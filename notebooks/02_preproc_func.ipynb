{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Preprocessing\n",
    "\n",
    "This notebooks preprocesses functional MRI images by executing the following processing steps:\n",
    "\n",
    "1. Reorient Images to RAS\n",
    "1. Removal of non-steady state volumes \n",
    "1. Motion Correction with SPM\n",
    "1. Slice-wise Correction with SPM\n",
    "1. Brain Extraction with SPM and FSL\n",
    "1. Temporal Filter with Nilearn\n",
    "1. Two- step coregistration using BBR with FSL, using WM segmentation from SPM\n",
    "1. Spatial Filter (i.e. smoothing) with Nilearn\n",
    "\n",
    "Additional, this workflow also performs:\n",
    " - Computes Friston's 24-paramter model for motion parameters\n",
    " - Computes Framewise Displacement (FD) and DVARS\n",
    " - Computes average signal in total volume, in GM, in WM and in CSF\n",
    " - Computes anatomical CompCor Components\n",
    " - Computes temporal CompCor Components\n",
    " \n",
    "**Note:** This notebook requires that the anatomical preprocessing pipeline was already executed and that it's output can be found in the dataset folder under `dataset/derivatives/fmriflows/preproc_anat`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── analysis-func_specs.json\n",
    "    ├── sub-{sub_id}\n",
    "    │   └── func\n",
    "    │       └── sub-{sub_id}_*task-{task_id}_run-{run_id}_bold.nii.gz\n",
    "    └── task-{task_id}_bold.json\n",
    "    \n",
    "**Note:** Subfolders for individual scan sessions are optional. `fmriflows` will run the preprocessing on all files of a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `analysis-func_specs.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'analysis-func_specs.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)\n",
    "\n",
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject names\n",
    "subject_list = specs['subject_list']\n",
    "\n",
    "# List of task names\n",
    "task_list = specs['task_list']\n",
    "\n",
    "# Mode and width of spatial filter, i.e. Low-Pass, fwhm of 6mm = ['LP', 6]\n",
    "filters_spatial = specs['filters_spatial']\n",
    "\n",
    "# High and low-pass filter to apply, i.e. Low-Pass of 100Hz = [\"None\", 100]\n",
    "filters_temporal = specs['filters_temporal']\n",
    "\n",
    "# Requested isometric voxel resolution after coregistration\n",
    "voxel_res = specs['voxel_res']\n",
    "\n",
    "# Reference time point (in ms) required for slice wise correction\n",
    "ref_time = specs['ref_timepoint']\n",
    "\n",
    "# Number of components to extract with ACompCor and TCompCor\n",
    "ncomp = specs['n_confound_comp']\n",
    "\n",
    "# Thresholds to use for outlier detection\n",
    "outlier_thr = specs['outlier_thresholds']\n",
    "\n",
    "# Number of cores to use\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Workflow\n",
    "\n",
    "To ensure a good overview of the functional preprocessing, the workflow was divided into three subworkflows:\n",
    "\n",
    "1. The Main Workflow, i.e. doing the actual preprocessing\n",
    "2. The Confound Workflow, i.e. computing confound variables\n",
    "3. Report Workflow, i.e. visualizating relevant steps for quality control\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "from nipype import Workflow, Node, MapNode, IdentityInterface, Function\n",
    "from nipype.interfaces.image import Reorient\n",
    "from nipype.interfaces.spm import SliceTiming, Realign\n",
    "from nipype.interfaces.fsl import FLIRT, MeanImage, BET, BinaryMaths, ExtractROI\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.algorithms.confounds import (ACompCor, TCompCor, NonSteadyStateDetector,\n",
    "                                         FramewiseDisplacement, ComputeDVARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-dev/spm12_mcr/spm/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Main Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient anatomical images to RAS\n",
    "reorient = Node(Reorient(orientation='RAS'), name='reorient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of Non-Steady State volumes\n",
    "nonsteady_detection = Node(NonSteadyStateDetector(), name='nonsteady_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store number of non-steady state volumes in text file\n",
    "def write_to_txt(in_file, n_volumes):\n",
    "    \n",
    "    import numpy as np\n",
    "    out_file = in_file.replace('.nii.gz', '_nss.txt')\n",
    "    np.savetxt(out_file, [n_volumes], fmt='%d')\n",
    "    return out_file\n",
    "\n",
    "write_nss = Node(Function(input_names=['in_file', 'n_volumes'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=write_to_txt),\n",
    "                 name='write_nss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Non-Steady State volumes\n",
    "nonsteady_removal = Node(ExtractROI(output_type='NIFTI',\n",
    "                                    t_size=-1),\n",
    "                         name='nonsteady_removal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence specifications of functional images\n",
    "def get_parameters(run_id):\n",
    "    \n",
    "    import json\n",
    "    import numpy as np\n",
    "    from os.path import join as opj\n",
    "    \n",
    "    task_id = run_id.split('_run')[0][5:]\n",
    "        \n",
    "    func_desc = opj('/data', 'task-%s_bold.json' % task_id)\n",
    "\n",
    "    with open(func_desc) as f:\n",
    "        func_desc = json.load(f)\n",
    "\n",
    "    # Read out relevant parameters\n",
    "    TR = func_desc['RepetitionTime']\n",
    "    slice_order = func_desc['SliceTiming']\n",
    "    nslices = len(slice_order)\n",
    "    time_acquisition = float(TR)-(TR/nslices)\n",
    "    \n",
    "    return TR, slice_order, nslices, time_acquisition\n",
    "\n",
    "getParam = Node(Function(input_names=['run_id'],\n",
    "                         output_names=['TR', 'slice_order',\n",
    "                                       'nslices', 'time_acquisition'],\n",
    "                         function=get_parameters),\n",
    "                name='getParam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for motion\n",
    "realign = Node(Realign(register_to_mean=True), name='realign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for slice-wise acquisition\n",
    "slicetime = Node(SliceTiming(ref_slice=ref_time), name='slicetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset TR value after SPM's slice time correction\n",
    "def add_TR_to_file(in_file, TR):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    \n",
    "    # Load image\n",
    "    img = nb.load(in_file)\n",
    "    \n",
    "    # Reset TR\n",
    "    img.header.set_zooms(list(img.header.get_zooms()[:3]) + [TR])\n",
    "    \n",
    "    # Save file\n",
    "    out_file = in_file.replace('.nii', '_TR.nii')\n",
    "    img.to_filename(out_file)\n",
    "    del img\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "reset_TR = Node(Function(input_names=['in_file', 'TR'],\n",
    "                         output_names=['out_file'],\n",
    "                         function=add_TR_to_file),\n",
    "                name='reset_TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove skull signal from functional images\n",
    "bet_func = Node(BET(functional=True,\n",
    "                    mask=True,\n",
    "                    output_type='NIFTI_GZ'),\n",
    "                name='bet_func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes mean image before coregistration\n",
    "bet_mean = Node(MeanImage(dimension='T',\n",
    "                          output_type='NIFTI_GZ'),\n",
    "                name='bet_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-alignment of functional images to anatomical image\n",
    "coreg_pre = Node(FLIRT(dof=6,\n",
    "                       output_type='NIFTI_GZ'),\n",
    "                 name='coreg_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coregistration of functional images to anatomical image with BBR\n",
    "# using WM segmentation\n",
    "coreg_bbr = Node(FLIRT(dof=6,\n",
    "                       cost='bbr',\n",
    "                       schedule=opj(os.getenv('FSLDIR'),\n",
    "                                    'etc/flirtsch/bbr.sch'),\n",
    "                       output_type='NIFTI_GZ'),\n",
    "                 name='coreg_bbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply coregistration warp to functional images\n",
    "applycoreg = Node(FLIRT(interp='spline',\n",
    "                        apply_isoxfm=voxel_res,\n",
    "                        datatype='short',\n",
    "                        output_type='NIFTI_GZ'),\n",
    "                 name='applycoreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Temporal Filter\n",
    "def apply_temporal_filter(in_file, TR, tFilter):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.image import clean_img, mean_img, math_img\n",
    "    \n",
    "    # Transform cutoff values into HZ\n",
    "    low_pass, high_pass = tFilter\n",
    "    postfix = 'tfilt_%s.%s' % (low_pass, high_pass)\n",
    "    low_pass = 1. / low_pass if low_pass != 'None' else None\n",
    "    high_pass = 1. / high_pass if high_pass != 'None' else None\n",
    "    \n",
    "    out_file = in_file.replace('.nii', '_%s.nii' % postfix)\n",
    "    \n",
    "    # Apply temporal filter and store it in new file\n",
    "    img = clean_img(in_file, detrend=False, standardize=False, t_r=TR,\n",
    "                    ensure_finite=True, low_pass=low_pass, high_pass=high_pass)\n",
    "    affine = img.affine\n",
    "    header = img.header\n",
    "\n",
    "    # Add mean if image was high pass filtered\n",
    "    if high_pass:\n",
    "        img = math_img(\"img1 + img2[...,None]\", img1=img, img2=mean_img(in_file))\n",
    "\n",
    "    # Save temporal filtered image\n",
    "    nb.Nifti1Image(img.get_data(), affine, header).to_filename(out_file)\n",
    "    del img\n",
    "\n",
    "    return out_file\n",
    "\n",
    "temporal_filter = Node(Function(input_names=['in_file', 'TR', 'tFilter'],\n",
    "                        output_names=['out_file'],\n",
    "                        function=apply_temporal_filter),\n",
    "               name='temp_filter')\n",
    "temporal_filter.iterables = ('tFilter', filters_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies gaussian spatial filter as in Sengupta, Pollmann & Hanke, 2018\n",
    "def gaussian_spatial_filter(in_file, sFilter, bandwidth=1):\n",
    "\n",
    "    import nibabel as nb\n",
    "    from nilearn.image import smooth_img\n",
    "\n",
    "    ftype, fwhm = sFilter\n",
    "\n",
    "    if ftype == 'LP':\n",
    "        img = smooth_img(in_file, fwhm=fwhm)\n",
    "        \n",
    "    if ftype == 'HP':\n",
    "        img = nb.load(in_file)\n",
    "        HPF_bold = img.get_data() - smooth_img(in_file, fwhm=fwhm).get_data()\n",
    "        img = nb.Nifti1Image(HPF_bold, img.get_affine())\n",
    "        \n",
    "    elif ftype == 'BP':\n",
    "        LPF_bold_1 = smooth_img(in_file, fwhm=fwhm)\n",
    "        LPF_bold_2 = smooth_img(in_file, fwhm=fwhm - bandwidth)\n",
    "        BPF_bold = LPF_bold_2.get_data() - LPF_bold_1.get_data()\n",
    "        img = nb.Nifti1Image(BPF_bold, LPF_bold_1.affine, LPF_bold_1.header)\n",
    "        \n",
    "    # Save and return output file\n",
    "    out_file = in_file.replace('.nii', '_%s_%smm.nii' % (ftype, fwhm))\n",
    "    img.to_filename(out_file)\n",
    "    del img\n",
    "\n",
    "    return out_file\n",
    "\n",
    "# Spatial Band-Pass Filter\n",
    "spatial_filter = Node(Function(input_names=['in_file', 'sFilter'],\n",
    "                        output_names=['out_file'],\n",
    "                        function=gaussian_spatial_filter),\n",
    "               name='spatial_filter')\n",
    "spatial_filter.iterables = ('sFilter', filters_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes mean image\n",
    "meanimg = Node(MeanImage(dimension='T',\n",
    "                         output_type='NIFTI_GZ'),\n",
    "               name='meanimg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Main Workflow\n",
    "\n",
    "**Note:** Slice time correction is applied after motion correction, as recommended by Power et al. (2017): http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0182939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main preprocessing workflow\n",
    "mainflow = Workflow(name='mainflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "mainflow.connect([(reorient, nonsteady_detection, [('out_file', 'in_file')]),\n",
    "                  (reorient, nonsteady_removal, [('out_file', 'in_file')]),\n",
    "                  (reorient, write_nss, [('out_file', 'in_file')]),\n",
    "                  (nonsteady_detection, write_nss, [('n_volumes_to_discard', 'n_volumes')]),\n",
    "                  (nonsteady_detection, nonsteady_removal, [('n_volumes_to_discard',\n",
    "                                                             't_min')]),\n",
    "                  (nonsteady_removal, realign, [('roi_file', 'in_files')]),\n",
    "                  (realign, slicetime, [('realigned_files', 'in_files')]),\n",
    "                  (getParam, slicetime, [('TR', 'time_repetition'),\n",
    "                                         ('slice_order', 'slice_order'),\n",
    "                                         ('nslices', 'num_slices'),\n",
    "                                         ('time_acquisition', 'time_acquisition'),\n",
    "                                         ]),\n",
    "                  (slicetime, reset_TR, [('timecorrected_files', 'in_file')]),\n",
    "                  (getParam, reset_TR, [('TR', 'TR')]),\n",
    "                  (reset_TR, bet_func, [('out_file', 'in_file')]),\n",
    "                  (bet_func, bet_mean, [('out_file', 'in_file')]),\n",
    "\n",
    "                  # Coregistration\n",
    "                  (coreg_pre, coreg_bbr, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                  (coreg_bbr, applycoreg, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                  (bet_mean, coreg_pre, [('out_file', 'in_file')]),\n",
    "                  (bet_mean, coreg_bbr, [('out_file', 'in_file')]),\n",
    "                  (bet_func, applycoreg, [('out_file', 'in_file')]),\n",
    "                  \n",
    "                  # Apply Temporal and Spatial Filter\n",
    "                  (getParam, temporal_filter, [('TR', 'TR')]),\n",
    "                  (applycoreg, temporal_filter, [('out_file', 'in_file')]),\n",
    "                  (temporal_filter, spatial_filter, [('out_file', 'in_file')]),\n",
    "                  (applycoreg, meanimg, [('out_file', 'in_file')]),\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Confound Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ACompCor (based on Behzadi et al., 2007)\n",
    "aCompCor = Node(ACompCor(num_components=ncomp,\n",
    "                         pre_filter=False,\n",
    "                         save_pre_filter=False,\n",
    "                         merge_method='union',\n",
    "                         components_file='compcorA.txt'),\n",
    "                name='aCompCor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary mask for ACompCor (based on Behzadi et al., 2007)\n",
    "def get_csf_wm_mask(in_file, wm, csf, brainmask):\n",
    "    \n",
    "    from nibabel import Nifti1Image, load\n",
    "    from nilearn.image import threshold_img, resample_to_img\n",
    "    from scipy.ndimage.morphology import binary_erosion, binary_closing\n",
    "\n",
    "    # Create eroded WM binary mask\n",
    "    thr_wm = threshold_img(wm, 0.99)\n",
    "    res_wm = resample_to_img(thr_wm, in_file)\n",
    "    bin_wm = threshold_img(res_wm, 0.5)\n",
    "    mask_wm = binary_erosion(bin_wm.get_data(), iterations=2).astype('int8')\n",
    "\n",
    "    # Create eroded CSF binary mask (differs from Behzadi et al., 2007)\n",
    "    thr_csf = threshold_img(csf, 0.99)\n",
    "    res_csf = resample_to_img(thr_csf, in_file)\n",
    "    bin_csf = threshold_img(res_csf, 0.5)\n",
    "    close_csf = binary_closing(bin_csf.get_data(), iterations=1)\n",
    "    mask_csf = binary_erosion(close_csf, iterations=1).astype('int8')\n",
    "    \n",
    "    # Combine WM and CSF binary masks into one and apply brainmask\n",
    "    mask_brain = load(brainmask).get_data()\n",
    "    binary_mask = (((mask_wm + mask_csf) * mask_brain) > 0).astype('int8')\n",
    "    out_file = in_file.replace('.nii', '_maskA.nii')\n",
    "    Nifti1Image(binary_mask, res_wm.affine).to_filename(out_file)\n",
    "\n",
    "    return out_file\n",
    "\n",
    "acomp_masks = Node(Function(input_names=['in_file', 'wm', 'csf', 'brainmask'],\n",
    "                            output_names=['out_file'],\n",
    "                            function=get_csf_wm_mask),\n",
    "                   name='acomp_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TCompCor (based on Behzadi et al., 2007)\n",
    "tCompCor = Node(TCompCor(num_components=ncomp,\n",
    "                         percentile_threshold=0.02,\n",
    "                         pre_filter=False,\n",
    "                         save_pre_filter=False,\n",
    "                         components_file='compcorT.txt'),\n",
    "                name='tCompCor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary mask for TCompCor approach (based on Behzadi et al., 2007)\n",
    "def get_brainmask(in_file):\n",
    "    \n",
    "    from nibabel import Nifti1Image\n",
    "    from nilearn.image import mean_img\n",
    "    from scipy.ndimage.morphology import binary_erosion\n",
    "    \n",
    "    img = mean_img(in_file)\n",
    "    erod_img = binary_erosion(img.get_data()>0, iterations=1).astype('int8')\n",
    "    \n",
    "    out_file = in_file.replace('.nii', '_maskT.nii')\n",
    "    Nifti1Image(erod_img, img.affine).to_filename(out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "tcomp_brainmask = Node(Function(input_names=['in_file'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=get_brainmask),\n",
    "                 name='tcomp_brainmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FramewiseDisplacement\n",
    "FD = Node(FramewiseDisplacement(parameter_source='SPM',\n",
    "                                normalize=False),\n",
    "          name='FD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DVARS\n",
    "dvars = Node(ComputeDVARS(remove_zerovariance=True,\n",
    "                          save_std=True),\n",
    "           name='dvars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes Friston 24-parameter model (Friston et al., 1996)\n",
    "def compute_friston24(in_file):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # Load raw motion parameters\n",
    "    mp_raw = np.loadtxt(in_file)\n",
    "    \n",
    "    # Get motion paremter one time point before\n",
    "    mp_minus1 = np.vstack((mp_raw[1:], [0] * 6))\n",
    "    \n",
    "    # Combine the two\n",
    "    mp_combine = np.hstack((mp_raw, mp_minus1))\n",
    "\n",
    "    # Add the square of those parameters to allow correction of nonlinear effects\n",
    "    mp_friston = np.hstack((mp_combine, mp_combine**2))\n",
    "\n",
    "    # Save friston 24-parameter model in new txt file\n",
    "    out_file = in_file.replace('.txt', 'friston24.txt')\n",
    "    np.savetxt(out_file, mp_friston,\n",
    "               fmt='%.8f', delimiter=' ', newline='\\n')\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "friston24 = Node(Function(input_names=['in_file'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=compute_friston24),\n",
    "                 name='friston24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average signal in total volume, in GM, in WM and in CSF\n",
    "def get_average_signal(in_file, gm, wm, csf, brainmask):\n",
    "    \n",
    "    from scipy.stats import zscore\n",
    "    from nilearn.masking import apply_mask\n",
    "    from nilearn.image import threshold_img, resample_to_img, math_img, mean_img\n",
    "\n",
    "    # Create masks for signal extraction\n",
    "    res_gm = resample_to_img(threshold_img(gm, 0.99), in_file)\n",
    "    bin_gm = math_img('img1>=0.5', img1=res_gm)\n",
    "\n",
    "    res_wm = resample_to_img(threshold_img(wm, 0.99), in_file)\n",
    "    bin_wm = math_img('img1>=0.5', img1=res_wm)\n",
    "\n",
    "    res_csf = resample_to_img(threshold_img(csf, 0.99), in_file)\n",
    "    bin_csf = math_img('img1>=0.5', img1=res_csf)\n",
    "\n",
    "    res_brain = resample_to_img(brainmask, in_file)\n",
    "    bin_brain = math_img('img1>=0.5', img1=res_brain)\n",
    "\n",
    "    # Compute average signal per mask and zscore timeserie\n",
    "    signal_gm = zscore(apply_mask(in_file, mask_img=bin_gm).mean(axis=-1))\n",
    "    signal_wm = zscore(apply_mask(in_file, mask_img=bin_wm).mean(axis=-1))\n",
    "    signal_csf = zscore(apply_mask(in_file, mask_img=bin_csf).mean(axis=-1))\n",
    "    signal_brain = zscore(apply_mask(in_file, mask_img=bin_brain).mean(axis=-1))\n",
    "\n",
    "    return [signal_brain, signal_gm, signal_wm, signal_csf]\n",
    "\n",
    "average_signal = Node(Function(input_names=['in_file', 'gm', 'wm', 'csf', 'brainmask'],\n",
    "                               output_names=['average'],\n",
    "                               function=get_average_signal),\n",
    "                      name='average_signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine confound parameters into one TSV file\n",
    "def consolidate(FD, DVARS, par_rp, par_friston, compA, compT, average):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    conf_FD = np.array([0] + list(np.loadtxt(FD, skiprows=1)))\n",
    "    conf_DVARS = np.array([1] + list(np.loadtxt(DVARS, skiprows=0)))\n",
    "    conf_rp = np.loadtxt(par_rp)\n",
    "    conf_friston = np.loadtxt(par_friston)\n",
    "    conf_compA = np.loadtxt(compA, skiprows=1)\n",
    "    conf_compT = np.loadtxt(compT, skiprows=1)\n",
    "    conf_average = np.array(average)\n",
    "\n",
    "    # Aggregate confounds\n",
    "    confounds = np.hstack((conf_FD[..., None],\n",
    "                           conf_DVARS[..., None],\n",
    "                           conf_average.T,\n",
    "                           conf_rp,\n",
    "                           conf_friston,\n",
    "                           conf_compA,\n",
    "                           conf_compT))\n",
    "\n",
    "    # Create header\n",
    "    header = ['FD', 'DVARS']\n",
    "    header += ['TV', 'GM', 'WM', 'CSF']\n",
    "    header += ['Motion%02d' % (d + 1) for d in range(conf_rp.shape[1])]\n",
    "    header += ['Friston%02d' % (d + 1) for d in range(conf_friston.shape[1])]\n",
    "    header += ['CompA%02d' % (d + 1) for d in range(conf_compA.shape[1])]\n",
    "    header += ['CompT%02d' % (d + 1) for d in range(conf_compT.shape[1])]\n",
    "\n",
    "    # Write to file\n",
    "    out_file = par_rp.replace('rp', 'confounds')\n",
    "    out_file = out_file.replace('.txt', '.tsv')\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write('\\t'.join(header) + '\\n')\n",
    "        for row in confounds:\n",
    "            f.write('\\t'.join([str(r) for r in row]) + '\\n')\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "combine_confounds = Node(Function(input_names=['FD', 'DVARS',\n",
    "                                               'par_rp', 'par_friston',\n",
    "                                               'compA', 'compT', 'average'],\n",
    "                                  output_names=['out_file'],\n",
    "                                  function=consolidate),\n",
    "                         name='combine_confounds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confound Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confound extraction workflow\n",
    "confflow = Workflow(name='confflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "confflow.connect([(acomp_masks, aCompCor, [('out_file', 'mask_files')]),\n",
    "                  (tcomp_brainmask, tCompCor, [('out_file', 'mask_files')]),\n",
    "                  (tcomp_brainmask, dvars, [('out_file', 'in_mask')]),\n",
    "                  (tcomp_brainmask, acomp_masks, [('out_file', 'brainmask')]),\n",
    "\n",
    "                  # Consolidate confounds\n",
    "                  (FD, combine_confounds, [('out_file', 'FD')]),\n",
    "                  (dvars, combine_confounds, [('out_std', 'DVARS')]),\n",
    "                  (aCompCor, combine_confounds, [('components_file', 'compA')]),\n",
    "                  (tCompCor, combine_confounds, [('components_file', 'compT')]),\n",
    "                  (friston24, combine_confounds, [('out_file', 'par_friston')]),\n",
    "                  (average_signal, combine_confounds, [('average', 'average')]),\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the report Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean image with ACompCor and TCompCor mask ovleray\n",
    "def plot_compcor_mask(sub_id, ses_id, run_id, mean, maskA, maskT):\n",
    "    \n",
    "    import numpy as np\n",
    "    import nibabel as nb\n",
    "    from matplotlib.pyplot import figure\n",
    "    from nilearn.plotting import plot_anat\n",
    "    from nilearn.image import coord_transform\n",
    "\n",
    "    # Support Function to get optimal cut for visualization\n",
    "    def get_cut_ids(img, axis=0):\n",
    "\n",
    "        # Compute voxel id to cut\n",
    "        idx = np.sort(img.get_data().nonzero()[axis])\n",
    "        vox_id = np.linspace(idx.min(), idx.max(), num=12, endpoint=True).astype('int')\n",
    "        vox_id = vox_id[2:-2]\n",
    "\n",
    "        # Translate voxel id to image space\n",
    "        if axis == 0:\n",
    "            cut_ids = [int(coord_transform(r, 0, 0, img.affine)[0]) for r in vox_id]\n",
    "        elif axis == 1:\n",
    "            cut_ids = [int(coord_transform(0, r, 0, img.affine)[1]) for r in vox_id]\n",
    "        elif axis == 2:\n",
    "            cut_ids = [int(coord_transform(0, 0, r, img.affine)[2]) for r in vox_id]\n",
    "        return cut_ids\n",
    "\n",
    "    # Visualize preprocessed functional mean on subject anatomy\n",
    "    def plot_mean(mean, maksA, maskT, title, out_file):\n",
    "        fig = figure(figsize=(16, 8))\n",
    "\n",
    "        for i, e in enumerate(['x', 'y', 'z']):\n",
    "            ax = fig.add_subplot(3, 1, i + 1)\n",
    "\n",
    "            display = plot_anat(mean, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                                display_mode=e, cut_coords=get_cut_ids(nb.load(mean), i),\n",
    "                                annotate=False, axes=ax)\n",
    "            display.add_overlay(maskA, cmap='plasma_r')\n",
    "            display.add_overlay(maskT, cmap='winter_r')\n",
    "        out_file = mean.replace('_mean.nii.gz', '_overlays.svg')\n",
    "        fig.savefig(out_file, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                    dpi=300, transparent=True)\n",
    "\n",
    "    task, run = run_id.split('_')\n",
    "    task = task[5:]\n",
    "    run = run[4:]\n",
    "\n",
    "    # If needed, create title for output figures\n",
    "    title_txt = 'Sub: %s - Task: %s - Run: %s' % (sub_id, task, run)\n",
    "    if ses_id:\n",
    "        title_txt = title_txt.replace('Run', 'Sess: %s - Run' % ses_id)\n",
    "    title_txt\n",
    "\n",
    "    # Establish name of output file\n",
    "    out_file = mean.replace('_mean.nii.gz', '_overlays.svg')\n",
    "\n",
    "    # Prepare maskA and maskT (otherwise they create strange looking outputs)\n",
    "    img = nb.load(mean)\n",
    "    imgA = nb.load(maskA)\n",
    "    maskA = nb.Nifti1Image(imgA.get_data()>0, img.affine, img.header)\n",
    "    imgT = nb.load(maskT)\n",
    "    maskT = nb.Nifti1Image(imgT.get_data()>0, img.affine, img.header)\n",
    "\n",
    "    # Create plot\n",
    "    plot_mean(mean, maskA, maskT, title_txt, out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "compcor_plot = Node(Function(input_names=['sub_id', 'ses_id', 'run_id',\n",
    "                                          'mean', 'maskA', 'maskT'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=plot_compcor_mask),\n",
    "                 name='compcor_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confounds and detect outliers\n",
    "def plot_confounds(confounds, outlier_thr):\n",
    "\n",
    "    # This plotting is heavily based on MRIQC's visual reports (credit to oesteban)\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.stats import zscore\n",
    "    from matplotlib.backends.backend_pdf import FigureCanvasPdf as FigureCanvas\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "\n",
    "    def plot_timeseries(dataframe, elements, out_file, outlier_thr=None):\n",
    "\n",
    "        # Number of rows to plot\n",
    "        n_rows = len(elements)\n",
    "\n",
    "        # Create canvas\n",
    "        fig = plt.Figure(figsize=(16, 2 * n_rows))\n",
    "        FigureCanvas(fig)\n",
    "        grid = GridSpec(n_rows, 2, width_ratios=[7, 1])\n",
    "\n",
    "        # Specify color palette to use\n",
    "        colors = sns.husl_palette(n_rows)\n",
    "\n",
    "        # To collect possible outlier indices\n",
    "        outlier_idx = []\n",
    "\n",
    "        # Plot timeseries (and detect outliers, if specified)\n",
    "        for i, e in enumerate(elements):\n",
    "\n",
    "            # Extract timeserie values\n",
    "            data = dataframe[e].values\n",
    "\n",
    "            # Z-score data for later thresholding\n",
    "            zdata = zscore(data)\n",
    "            \n",
    "            # Plot timeserie\n",
    "            ax = fig.add_subplot(grid[i, :-1])\n",
    "            ax.plot(data, color=colors[i])\n",
    "            ax.set_xlim((0, len(data)))\n",
    "            ax.set_ylabel(e)\n",
    "            ylim = ax.get_ylim()\n",
    "\n",
    "            # Detect and plot outliers if threshold is specified\n",
    "            if outlier_thr:\n",
    "\n",
    "                threshold = outlier_thr[i]\n",
    "\n",
    "                if threshold != 'None':\n",
    "\n",
    "                    outlier_id = np.where(np.abs(zdata)>=threshold)[0]\n",
    "                    outlier_idx += list(outlier_id)\n",
    "                    ax.vlines(outlier_id, ylim[0], ylim[1])\n",
    "\n",
    "            # Plot observation distribution\n",
    "            ax = fig.add_subplot(grid[i, -1])\n",
    "            sns.distplot(data, vertical=True, ax=ax, color=colors[i])\n",
    "            ax.set_ylim(ylim)\n",
    "\n",
    "        fig.savefig(out_file)\n",
    "\n",
    "        return np.unique(outlier_idx)\n",
    "\n",
    "    # Load confounds table\n",
    "    df = pd.read_table(confounds)\n",
    "\n",
    "    # Aggregate output plots\n",
    "    out_plots = []\n",
    "    \n",
    "    # Plot main confounds\n",
    "    elements = ['FD', 'DVARS', 'TV', 'GM', 'WM', 'CSF']\n",
    "    out_file = confounds.replace('.tsv', '_main.svg')\n",
    "    out_plots.append(out_file)\n",
    "    outliers = plot_timeseries(df, elements, out_file, outlier_thr)\n",
    "    \n",
    "    # Save outlier indices to textfile\n",
    "    outlier_filename = confounds.replace('.tsv', '_outliers.txt')\n",
    "    np.savetxt(outlier_filename, outliers, fmt='%d')\n",
    "\n",
    "    # Plot Motion Paramters\n",
    "    elements = [k for k in df.keys() if 'Motion' in k]\n",
    "    out_file = confounds.replace('.tsv', '_motion.svg')\n",
    "    out_plots.append(out_file)\n",
    "    plot_timeseries(df, elements, out_file)\n",
    "\n",
    "    # Plot CompCor components\n",
    "    for comp in ['A', 'T']:\n",
    "        elements = [k for k in df.keys() if 'Comp%s' % comp in k]\n",
    "        out_file = confounds.replace('.tsv', '_comp%s.svg' % comp)\n",
    "        out_plots.append(out_file)\n",
    "        plot_timeseries(df, elements, out_file)\n",
    "    \n",
    "    return [outlier_filename] + out_plots\n",
    "\n",
    "confound_inspection = Node(Function(input_names=['confounds', 'outlier_thr'],\n",
    "                                    output_names=['out_file', 'plot_main', 'plot_motion',\n",
    "                                                  'plot_compA', 'plot_compT'],\n",
    "                                    function=plot_confounds),\n",
    "                           name='confound_inspection')\n",
    "confound_inspection.inputs.outlier_thr = outlier_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update report\n",
    "def write_report(sub_id, ses_id, run_list, cor_plot, conf_plot):\n",
    "\n",
    "    # Get list of all individual tasks and runs\n",
    "    func_idx = [run_id[5:].split('_run-') for run_id in run_list]\n",
    "\n",
    "    # Load template for functional preprocessing output\n",
    "    with open('/templates/report_template_func.html', 'r') as report:\n",
    "        func_temp = report.read()\n",
    "\n",
    "    # Get filename of html report\n",
    "    if ses_id:\n",
    "        html_file = '/data/derivatives/fmriflows/sub-%s_ses-%s.html' % (sub_id, ses_id)\n",
    "    else:\n",
    "        html_file = '/data/derivatives/fmriflows/sub-%s.html' % sub_id\n",
    "\n",
    "    # Old template placeholder\n",
    "    func_key = '<p>The functional preprocessing pipeline hasn\\'t been run yet.</p>'\n",
    "    \n",
    "    # Add new content to report\n",
    "    with open(html_file, 'r') as report:\n",
    "        txt = report.read()\n",
    "        \n",
    "        # Reset report with functional preprocessing template\n",
    "        cut_start = txt.find('Functional Preprocessing</a></h2>') + 33\n",
    "        cut_stop = txt.find('<!-- Section: 1st-Level Univariate Results-->')\n",
    "        txt = txt[:cut_start] + func_key + txt[cut_stop:]\n",
    "\n",
    "        txt_amendment = ''\n",
    "\n",
    "        # Go through the tasks and runs\n",
    "        for task_id, run_id in func_idx:\n",
    "\n",
    "            func_txt = func_temp.replace('sub-placeholder', 'sub-%s' % sub_id)\n",
    "            func_txt = func_txt.replace('task-placeholder', 'task-%s' % task_id)\n",
    "            func_txt = func_txt.replace('run-placeholder', 'run-%s' % run_id)\n",
    "\n",
    "            # Add session suffix if present\n",
    "            if ses_id:\n",
    "                func_txt = func_txt.replace('ses-placeholder', 'ses-%s' % ses_id)\n",
    "            else:\n",
    "                func_txt = func_txt.replace('ses-placeholder', '')\n",
    "                func_txt = func_txt.replace('__', '_')\n",
    "\n",
    "            txt_amendment += func_txt\n",
    " \n",
    "    # Add pipeline graphs\n",
    "    txt_amendment += '<h3 class=\"h3\" style=\"position:left;font-weight:bold\">Graph of'\n",
    "    txt_amendment += ' Functional Preprocessing pipeline</h3>\\n    <object data=\"preproc_func/graph.svg\"'\n",
    "    txt_amendment += ' type=\"image/svg+xml\" style=\"width:100%\"></object>\\n  '\n",
    "    txt_amendment += ' <object data=\"preproc_func/graph_detailed.svg\" type=\"image/svg+xml\"'\n",
    "    txt_amendment += ' style=\"width:100%\"></object>\\n'\n",
    "\n",
    "    # Insert functional preprocessing report\n",
    "    txt = txt.replace(func_key, txt_amendment)\n",
    "\n",
    "    # Overwrite previous report\n",
    "    with open(html_file, 'w') as report:\n",
    "        report.writelines(txt)\n",
    "\n",
    "create_report = MapNode(Function(input_names=['sub_id', 'ses_id', 'run_list',\n",
    "                                              'cor_plot', 'conf_plot'],\n",
    "                                 output_names=['out_file'],\n",
    "                                 function=write_report),\n",
    "                     name='create_report', iterfield=['cor_plot', 'conf_plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create report Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create report workflow\n",
    "reportflow = Workflow(name='reportflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "reportflow.connect([(compcor_plot, create_report, [('out_file', 'cor_plot')]),\n",
    "                    (confound_inspection, create_report, [('plot_main', 'conf_plot')])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all functional files\n",
    "from bids.grabbids import BIDSLayout\n",
    "layout = BIDSLayout('/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get session name if it exists\n",
    "session_list = layout.get_sessions()\n",
    "session_list = session_list if session_list else ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name specifier for runs per task\n",
    "run_list = [(l.task, 'task-%s_run-%02d' % (l.task, int(l.run)))\n",
    "            for l in layout.get(modality='func')]\n",
    "\n",
    "# Keep only tasks that are specified in task_list\n",
    "run_list = np.unique([r[1] for r in run_list if r[0] in task_list]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add run_list as input to create_report node\n",
    "create_report.inputs.run_list = run_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject, session, task and run id\n",
    "infosource = Node(IdentityInterface(fields=['subject_id', 'session_id', 'run_id']),\n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('session_id', session_list),\n",
    "                        ('run_id', run_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def create_file_path(subject_id, session_id, run_id, layout):\n",
    "\n",
    "    from os.path import join\n",
    "    \n",
    "    entities = {'subject_id': subject_id,\n",
    "                'run_id': run_id}\n",
    "    \n",
    "    # Add session id if present in dataset\n",
    "    if session_id != '':\n",
    "        entities['session_id'] = session_id\n",
    "\n",
    "    # Collect input files\n",
    "    in_files = []\n",
    "\n",
    "    # Get functional image\n",
    "    pattern = '/data/sub-{subject_id}[/ses-{session_id}]/func/'\n",
    "    pattern += 'sub-{subject_id}[_ses-{session_id}]_{run_id}_bold.nii.gz'\n",
    "    fpath = layout.build_path(entities, path_patterns=[pattern])\n",
    "    in_files.append(join('/data', fpath))\n",
    "    \n",
    "    # Get anatomical images\n",
    "    for t in ['brain.nii.gz', 'seg_gm.nii', 'seg_wm.nii', 'seg_csf.nii', 'brainmask.nii.gz']:\n",
    "        pattern = 'sub-{subject_id}/sub-{subject_id}[_ses-{session_id}]_%s' % t\n",
    "        fpath = layout.build_path(entities, path_patterns=[pattern])\n",
    "        fpath = join('/data/derivatives/fmriflows/preproc_anat', fpath)\n",
    "        in_files.append(fpath)\n",
    "\n",
    "    return in_files\n",
    "\n",
    "selectfiles = Node(Function(input_names=['subject_id', 'session_id',\n",
    "                                         'run_id',  'layout'],\n",
    "                            output_names=['func', 'brain', 'gm', 'wm', 'csf', 'brainmask'],\n",
    "                            function=create_file_path),\n",
    "                   name='selectfiles')\n",
    "selectfiles.inputs.layout = layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [(\n",
    "    '_run_id_%s_session_id_%s_subject_id_%s/' % (run, sess, sub),\n",
    "    'sub-%s/sub-%s_ses-%s_%s_' % (sub, sub, sess, run))\n",
    "    for sub in subject_list\n",
    "    for sess in session_list\n",
    "    for run in run_list]\n",
    "substitutions += [\n",
    "    ('sub-%s_%s_bold' % (sub, run), '')\n",
    "    for sub in subject_list\n",
    "    for run in run_list]\n",
    "substitutions += [\n",
    "    ('sub-%s_ses-%s_%s_bold' % (sub, sess, run), '')\n",
    "    for sub in subject_list\n",
    "    for sess in session_list\n",
    "    for run in run_list]\n",
    "substitutions += [('ar_', ''),\n",
    "                  ('ras_', ''),\n",
    "                  ('roi_', ''),\n",
    "                  ('TR_', ''),\n",
    "                  ('brain_', ''),\n",
    "                  ('flirt_', ''),\n",
    "                  ('tfilt', 'tFilter'),\n",
    "                  ('mask_000', 'maskT'),\n",
    "                  ('_roi', '_'),\n",
    "                  ('__', '_'),\n",
    "                  ('_.', '.'),\n",
    "                  ('ses-_', ''),\n",
    "                  ]\n",
    "substitutions += [('%s_%smm' % (s[0], s[1]),\n",
    "                   'sFilter_%s_%smm' % (s[0], s[1]))\n",
    "                  for s in filters_spatial]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implement Functional Preprocessing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functional preprocessing workflow\n",
    "preproc_func = Workflow(name='preproc_func')\n",
    "preproc_func.base_dir = work_dir\n",
    "\n",
    "# Connect input nodes to each other\n",
    "preproc_func.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                 ('session_id', 'session_id'),\n",
    "                                                 ('run_id', 'run_id')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the main workflow\n",
    "preproc_func.connect([(infosource, mainflow, [('run_id', 'getParam.run_id')]),\n",
    "                      (selectfiles, mainflow, [('func', 'reorient.in_file'),\n",
    "                                               ('brain', 'coreg_pre.reference'),\n",
    "                                               ('brain', 'coreg_bbr.reference'),\n",
    "                                               ('wm', 'coreg_bbr.wm_seg'),\n",
    "                                               ('brain', 'applycoreg.reference'),\n",
    "                                               ]),\n",
    "                      \n",
    "                      (mainflow, datasink, [\n",
    "                          ('spatial_filter.out_file', 'preproc_func.@func'),\n",
    "                          ('realign.realignment_parameters', 'preproc_func.@rp_par'),\n",
    "                          ('meanimg.out_file', 'preproc_func.@mean'),\n",
    "                          ('write_nss.out_file', 'preproc_func.@nss')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the confound workflow\n",
    "preproc_func.connect([(selectfiles, confflow, [('gm', 'average_signal.gm'),\n",
    "                                               ('wm', 'average_signal.wm'),\n",
    "                                               ('csf', 'average_signal.csf'),\n",
    "                                               ('brainmask', 'average_signal.brainmask'),\n",
    "                                               ('wm', 'acomp_masks.wm'),\n",
    "                                               ('csf', 'acomp_masks.csf')]),\n",
    "\n",
    "                      (confflow, datasink, [\n",
    "                          ('tCompCor.high_variance_masks', 'preproc_func.@maskT'),\n",
    "                          ('acomp_masks.out_file', 'preproc_func.@maskA'),\n",
    "                          ('combine_confounds.out_file', 'preproc_func.@confound_tsv')\n",
    "                      ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Connect main workflow with confound workflow\n",
    "preproc_func.connect([(mainflow, confflow, [\n",
    "                          ('getParam.TR', 'aCompCor.repetition_time'),\n",
    "                          ('applycoreg.out_file', 'aCompCor.realigned_file'),\n",
    "                          ('applycoreg.out_file', 'acomp_masks.in_file'),\n",
    "                          ('getParam.TR', 'tCompCor.repetition_time'),\n",
    "                          ('applycoreg.out_file', 'tCompCor.realigned_file'),\n",
    "                          ('applycoreg.out_file', 'tcomp_brainmask.in_file'),\n",
    "                          ('applycoreg.out_file', 'average_signal.in_file'),\n",
    "    \n",
    "                          ('realign.realignment_parameters', 'combine_confounds.par_rp'),\n",
    "                          ('realign.realignment_parameters', 'friston24.in_file'),\n",
    "                          ('realign.realignment_parameters', 'FD.in_file'),\n",
    "                          ('getParam.TR', 'FD.series_tr'),\n",
    "                          ('applycoreg.out_file', 'dvars.in_file'),\n",
    "                          ('getParam.TR', 'dvars.series_tr'),\n",
    "                          ])\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the report workflow\n",
    "preproc_func.connect([(infosource, reportflow, [('subject_id', 'compcor_plot.sub_id'),\n",
    "                                                ('session_id', 'compcor_plot.ses_id'),\n",
    "                                                ('run_id', 'compcor_plot.run_id'),\n",
    "                                                ('subject_id', 'create_report.sub_id'),\n",
    "                                                ('session_id', 'create_report.ses_id')\n",
    "                                               ]),\n",
    "\n",
    "                      (reportflow, datasink, [\n",
    "                          ('compcor_plot.out_file', 'preproc_func.@compcor_plot'),\n",
    "                          ('confound_inspection.out_file', 'preproc_func.@conf_inspect'),\n",
    "                          ('confound_inspection.plot_main', 'preproc_func.@conf_main'),\n",
    "                          ('confound_inspection.plot_motion', 'preproc_func.@conf_motion'),\n",
    "                          ('confound_inspection.plot_compA', 'preproc_func.@conf_compA'),\n",
    "                          ('confound_inspection.plot_compT', 'preproc_func.@conf_compT')                          \n",
    "                      ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect main and confound workflow with report workflow\n",
    "preproc_func.connect([(mainflow, reportflow, [\n",
    "                          ('meanimg.out_file', 'compcor_plot.mean')\n",
    "                          ]),\n",
    "                      (confflow, reportflow, [\n",
    "                          ('tCompCor.high_variance_masks', 'compcor_plot.maskT'),\n",
    "                          ('acomp_masks.out_file', 'compcor_plot.maskA'),\n",
    "                          ('combine_confounds.out_file', 'confound_inspection.confounds'),\n",
    "                          ])\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create preproc_func output graph\n",
    "preproc_func.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import SVG\n",
    "SVG(filename=opj(preproc_func.base_dir, 'preproc_func', 'graph.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow in parallel mode\n",
    "preproc_func.run(plugin='MultiProc', plugin_args={'n_procs' : n_proc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "preproc_func.write_graph(graph2use='flat', format='svg', simple_form=True)\n",
    "preproc_func.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(preproc_func.base_dir, 'preproc_func', 'graph.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_func', 'graph.svg'))\n",
    "copyfile(opj(preproc_func.base_dir, 'preproc_func', 'graph_detailed.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_func', 'graph_detailed.svg'));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
