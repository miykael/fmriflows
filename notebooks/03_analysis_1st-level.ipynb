{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style=\"float: right;\"><img width=\"66%\" src=\"templates/logo_fmriflows.gif\"></p>\n",
    "    <h1>1st-level Analysis</h1>\n",
    "    <p>This notebook performes the 1st-level analysis in subject space by executing the following steps:\n",
    "\n",
    "1. Aggregate 1st-level model parameters\n",
    "2. Specify 1st-level contrasts\n",
    "3. Estimate 1st-level contrasts\n",
    "4. Normalization to template space (optional)\n",
    "\n",
    "The notebook runs on all functional runs of a particular task, computes the specified beta-contrast and normalizes them into template space. If requested, it will also compute one beta contrast per condition, per run (usefull for a multivariate approache). Confounding factors and outlier volumes can be added as nuisance regressors in the GLM.\n",
    "\n",
    "**Note:** This notebook requires that the functional preprocessing pipeline was already executed and that it's output can be found in the dataset folder under `dataset/derivatives/fmriflows/preproc_func`. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── fmriflows_spec_analysis.json\n",
    "    ├── sub-{sub_id}\n",
    "    │   └── func\n",
    "    │       └── sub-{sub_id}_task-{task_id}[_run-{run_id}]_events.tsv\n",
    "    └── derivatives\n",
    "        └── fmriflows\n",
    "            ├── preproc_anat\n",
    "            │   └── sub-{sub_id}\n",
    "            │       └── {sub_id}[_run-{run_id}]_transformComposite.h5\n",
    "            └── preproc_func\n",
    "                └── sub-{sub_id}\n",
    "                    ├── sub-{sub_id}_task-{task_id}[_run-{run_id}]_nss.txt\n",
    "                    ├── sub-{sub_id}_task-{task_id}[_run-{run_id}]_tFilter_*_confounds.tsv\n",
    "                    ├── sub-{sub_id}_task-{task_id}[_run-{run_id}]_tFilter_*_confounds_outliers.txt\n",
    "                    └── sub-{sub_id}_task-{task_id}[_run-{run_id}]_tFilter_*_sFilter_*.nii.gz\n",
    "                    \n",
    "**Note:** `Session` and `run` identifiers are optional.\n",
    "\n",
    "`fmriflows` will run the 1st-level analysis on all runs, but separately for each session and particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant analysis specifications from the `fmriflows_spec_analysis.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_analysis.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for 1st-level analysis workflow\n",
    "subject_list = specs['subject_list']\n",
    "session_list = specs['session_list']\n",
    "tasks = specs['tasks']\n",
    "filters_spatial = specs['filters_spatial']\n",
    "filters_temporal = specs['filters_temporal']\n",
    "nuisance_regressors = specs['nuisance_regressors']\n",
    "use_outliers = specs['use_outliers']\n",
    "model_serial_correlations = specs['model_serial_correlations']\n",
    "model_bases = specs['model_bases']\n",
    "estimation_method = specs['estimation_method']\n",
    "normalize = specs['normalize']\n",
    "norm_res = specs['norm_res']\n",
    "con_per_run = specs['con_per_run']\n",
    "norm_res_multi = specs['norm_res_multi']\n",
    "postfix = specs['analysis_postfix']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of spatial filters (smoothing) that were used during functional preprocessing\n",
    "filters_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temporal filters that were used during functional preprocessing\n",
    "filters_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuisance identifiers that should be included in the GLM\n",
    "nuisance_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If outliers detected during functional preprocing should be used in GLM\n",
    "use_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial Correlation Model to use: 'AR(1)', 'FAST' or 'none'\n",
    "model_serial_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bases to use: 'hrf', 'fourier', 'fourier', 'fourier_han', 'gamma' or 'fir'\n",
    "# If 'hrf', also specify if time and dispersion derivatives should be used\n",
    "model_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation Method to use: 'Classical', 'Bayesian' or 'Bayesian2'\n",
    "estimation_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if contrasts should be normalized to template space after estimation\n",
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify voxel resolution of normalized contrasts\n",
    "norm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if a contrast should be computed for stimuli category per run\n",
    "con_per_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify voxel resolution of normalized contrasts for multivariate analysis\n",
    "norm_res_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a particular analysis postfix\n",
    "postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task specific parameters\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Workflow\n",
    "\n",
    "To ensure a good overview of the 1st-level analysis, the workflow was divided into an analysis and a report subworkflow.\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, MapNode, Workflow\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.algorithms.modelgen import SpecifySPMModel\n",
    "from nipype.interfaces.spm import Level1Design, EstimateModel, EstimateContrast\n",
    "from nipype.interfaces.ants import ApplyTransforms\n",
    "from nipype.interfaces.utility import Merge\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-r7219/spm12_mcr/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify 1st-level model parameters (stimuli onsets, duration, etc.)\n",
    "def collect_model_info(event_files, nss_files, condition_names):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "\n",
    "    model_info = []\n",
    "    stimuli_order = []\n",
    "\n",
    "    for i, f in enumerate(event_files):\n",
    "\n",
    "        trialinfo = pd.read_table(f)\n",
    "        stimuli_list = [t for t in trialinfo.trial_type if str(t) in condition_names]\n",
    "        stimuli_order.append(stimuli_list)\n",
    "        nss = np.loadtxt(nss_files[i])\n",
    "        conditions = []\n",
    "        onsets = []\n",
    "        durations = []\n",
    "\n",
    "        for group in trialinfo.groupby('trial_type'):\n",
    "            if str(group[0]) in condition_names:\n",
    "                conditions.append(str(group[0]))\n",
    "                onsets.append(list(group[1].onset - nss))\n",
    "                durations.append(group[1].duration.tolist())\n",
    "\n",
    "        model_info.append(Bunch(conditions=conditions,\n",
    "                                onsets=onsets,\n",
    "                                durations=durations))\n",
    "   \n",
    "    return model_info, stimuli_order\n",
    "\n",
    "# Get Subject Info - get subject specific condition information\n",
    "get_model_info = Node(Function(input_names=['event_files', 'nss_files', 'condition_names'],\n",
    "                               output_names=['model_info', 'stimuli_order'],\n",
    "                               function=collect_model_info),\n",
    "                      name='get_model_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzip NIfTI files for SPM\n",
    "gunzip = MapNode(Gunzip(), name='gunzip', iterfield=['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nuisance regressors\n",
    "def create_nuisance_regressors(confounds, nuisance_regressors):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from os.path import basename, abspath\n",
    "\n",
    "    # To store regressor files into\n",
    "    regressor_files = []\n",
    "\n",
    "    # Go through confound files\n",
    "    for i, c in enumerate(confounds):\n",
    "        df = pd.read_table(c)\n",
    "        selection = [k for k in df.keys() for n in nuisance_regressors if n in k]\n",
    "        dfs = df[selection]\n",
    "        out_file = abspath(basename('confounds_%02d.rst' % (i + 1)))\n",
    "        np.savetxt(out_file, dfs.values)\n",
    "        regressor_files.append(out_file)\n",
    "\n",
    "    return regressor_files\n",
    "\n",
    "nuisance_reg = Node(Function(input_names=['confounds', 'nuisance_regressors'],\n",
    "                             output_names=['confounds'],\n",
    "                             function=create_nuisance_regressors),\n",
    "                      name='nuisance_reg')\n",
    "nuisance_reg.inputs.nuisance_regressors = nuisance_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SPM model\n",
    "model_spec = Node(SpecifySPMModel(concatenate_runs=False,\n",
    "                                  input_units='secs',\n",
    "                                  output_units='secs'),\n",
    "                  name=\"model_spec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1st-level desing\n",
    "level1_design = Node(Level1Design(bases=model_bases,\n",
    "                                  timing_units='secs',\n",
    "                                  model_serial_correlations=model_serial_correlations),\n",
    "                    name=\"level1_design\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 1st-level model\n",
    "level1_estimate = Node(EstimateModel(estimation_method=estimation_method),\n",
    "                       name=\"level1_estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 1st-level contrasts\n",
    "level1_con_est = Node(EstimateContrast(), name=\"level1_con_est\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis workflow\n",
    "analysisflow = Workflow(name='analysisflow')\n",
    "\n",
    "# Add nodes to workflow and connect them\n",
    "analysisflow.connect([(get_model_info, model_spec, [('model_info', 'subject_info')]),\n",
    "                      (gunzip, model_spec, [('out_file', 'functional_runs')]),\n",
    "                      (nuisance_reg, model_spec, [('confounds', 'realignment_parameters')]),\n",
    "                      (model_spec, level1_design, [('session_info', 'session_info')]),\n",
    "                      (level1_design, level1_estimate, [('spm_mat_file', 'spm_mat_file')]),\n",
    "                      (level1_estimate, level1_con_est, [('spm_mat_file', 'spm_mat_file'),\n",
    "                                                         ('beta_images', 'beta_images'),\n",
    "                                                         ('residual_image', 'residual_image')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Report Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots design matrix\n",
    "def plot_design_matrix(SPM):\n",
    "\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    from scipy.io import loadmat\n",
    "    from os.path import basename, abspath\n",
    "\n",
    "    # Using scipy's loadmat function we can access SPM.mat\n",
    "    spmmat = loadmat(SPM, struct_as_record=False)\n",
    "    \n",
    "    # Now we can load the design matrix and the names of the rows\n",
    "    designMatrix = spmmat['SPM'][0][0].xX[0][0].X\n",
    "    names = [i[0] for i in spmmat['SPM'][0][0].xX[0][0].name[0]]\n",
    "\n",
    "    # Value normalization for better visualization\n",
    "    normed_design = designMatrix / np.abs(designMatrix).max(axis=0)\n",
    "\n",
    "    # Plotting of the design matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.imshow(normed_design, aspect='auto', cmap='gray', interpolation='nearest')\n",
    "    ax.set_ylabel('Volume id')\n",
    "    ax.set_xticks(np.arange(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=90)\n",
    "    design_matrix = abspath(basename(SPM.replace('.mat', '.png')))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(design_matrix)\n",
    "    \n",
    "    return design_matrix\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "plot_GLM = Node(Function(input_names=['SPM'],\n",
    "                         output_names=['out_file'],\n",
    "                         function=plot_design_matrix),\n",
    "                name='plot_GLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contrast image\n",
    "def plot_contrast(con_files, contrast_list, threshold):\n",
    "\n",
    "    from nilearn.image import math_img\n",
    "    from nilearn.plotting import plot_glass_brain\n",
    "    from os.path import basename, abspath\n",
    "\n",
    "    out_files = []\n",
    "    for i, c in enumerate(con_files):\n",
    "        percentile_str = 'np.percentile(np.abs(img[np.nan_to_num(img)!=0]), %s)' % threshold\n",
    "        con_thr = math_img('img * (np.abs(np.nan_to_num(img))>=%s)' % percentile_str, img=c)\n",
    "        title = contrast_list[i][0]\n",
    "        new_file_name = c.replace('.nii', '.png').replace('.gz', '')\n",
    "        out_file = abspath(basename(new_file_name))\n",
    "        plot_glass_brain(con_thr, display_mode='lyrz', colorbar=True,\n",
    "                         symmetric_cbar=False, plot_abs=False, black_bg=True,\n",
    "                         title=title, output_file=out_file)\n",
    "        out_files.append(out_file)\n",
    "    \n",
    "    return out_files\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "plot_contrasts = Node(Function(input_names=['con_files', 'contrast_list', 'threshold'],\n",
    "                               output_names=['out_files'],\n",
    "                               function=plot_contrast),\n",
    "                      name='plot_contrasts')\n",
    "plot_contrasts.inputs.threshold = 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create report workflow\n",
    "reportflow = Workflow(name='reportflow')\n",
    "\n",
    "# Add nodes to workflow and connect them\n",
    "reportflow.add_nodes([plot_GLM,\n",
    "                      plot_contrasts,\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject, session, task and run id\n",
    "info_source = Node(IdentityInterface(fields=['subject_id',\n",
    "                                             'session_id',\n",
    "                                             'task_id',\n",
    "                                             'spatial_filt',\n",
    "                                             'temporal_filt']),\n",
    "                   name='info_source')\n",
    "\n",
    "iter_list = [('subject_id', subject_list),\n",
    "             ('task_id', list(tasks.keys())),\n",
    "             ('spatial_filt', filters_spatial),\n",
    "             ('temporal_filt', filters_temporal),\n",
    "             ]\n",
    "\n",
    "if session_list:\n",
    "    iter_list.append(('session_id', session_list))\n",
    "else:\n",
    "    info_source.inputs.session_id = ''\n",
    "\n",
    "info_source.iterables = iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to input files\n",
    "def create_file_path(subject_id, session_id, task_id, tFilter, sFilter):\n",
    "\n",
    "    # Get all anatomical files\n",
    "    from bids.layout import BIDSLayout\n",
    "    layout = BIDSLayout('/data/')\n",
    "\n",
    "    search_parameters = {'subject': subject_id,\n",
    "                         'return_type': 'file',\n",
    "                        }\n",
    "    if session_id:\n",
    "        search_parameters['session'] = session_id\n",
    "        \n",
    "    # Collect normalization matrix\n",
    "    transforms = layout.get(**search_parameters, type='transformComposite')[0]\n",
    "    \n",
    "    # Collect outputs from the functional preprocessing\n",
    "    nss = []\n",
    "    confounds = []\n",
    "    outliers = []\n",
    "    func = []\n",
    "\n",
    "    derivatives = layout.get(**search_parameters, task=task_id)\n",
    "    for d in derivatives:\n",
    "\n",
    "        tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "        sFilter_id = '%s_%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "        if '/workingdir/' in d:\n",
    "            continue\n",
    "        elif 'nss.txt' in d:\n",
    "            nss.append(d)\n",
    "        elif tFilter_id in d:\n",
    "            if 'confounds.tsv' in d:\n",
    "                confounds.append(d)\n",
    "            elif 'confounds_outliers.txt' in d:\n",
    "                outliers.append(d)\n",
    "            elif sFilter_id in d and 'mm.nii' in d:\n",
    "                func.append(d)\n",
    "\n",
    "    # Collect event files\n",
    "    search_parameters = {'task': task_id,\n",
    "                         'return_type': 'file',\n",
    "                         'type': 'events'\n",
    "                        }\n",
    "\n",
    "    if session_id:\n",
    "        search_parameters['session'] = session_id\n",
    "\n",
    "    event_files = layout.get(**search_parameters)\n",
    "    if len(event_files) == 1:\n",
    "        events = [event_files[0]] * len(func)\n",
    "    else:\n",
    "        events = []\n",
    "        for e in event_files:\n",
    "            if 'sub-%s' % subject_id in e:\n",
    "                events.append(e)\n",
    "\n",
    "    return transforms, sorted(func), sorted(events), sorted(outliers), sorted(confounds), sorted(nss)\n",
    "\n",
    "select_files = Node(Function(input_names=['subject_id', 'session_id', 'task_id',\n",
    "                                          'tFilter', 'sFilter'],\n",
    "                             output_names=['transforms', 'func', 'events',\n",
    "                                           'outliers', 'confounds', 'nss'],\n",
    "                             function=create_file_path),\n",
    "                    name='select_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence specifications of functional images\n",
    "def get_parameters(func, tFilter, tasks, task_id):\n",
    "    \n",
    "    from nibabel import load\n",
    "    \n",
    "    # Extract TR from first functional image\n",
    "    TR = load(func[0]).header.get_zooms()[3]\n",
    "    \n",
    "    # Specify high pass filter\n",
    "    high_pass = tFilter[1] if not None else 128.\n",
    "\n",
    "    # Extract contrasts from specification file\n",
    "    contrast_list = tasks[task_id]\n",
    "    contrasts = [[c[0], c[2], contrast_list['condition_names'], c[1]]\n",
    "                 for c in contrast_list['contrasts']]\n",
    "\n",
    "    return TR, high_pass, contrasts, contrast_list['condition_names']\n",
    "\n",
    "get_param = Node(Function(input_names=['func', 'tFilter', 'tasks', 'task_id'],\n",
    "                          output_names=['TR', 'high_pass', 'contrasts', 'condition_names'],\n",
    "                          function=get_parameters),\n",
    "                 name='get_param')\n",
    "get_param.inputs.tasks = tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "if session_list:\n",
    "\n",
    "    folder_old = ['_session_id_%s_spatial_filt_%s_subject_id_%s_task_id_%s_temporal_filt_%s/' % (\n",
    "        ses, '.'.join([str(f) for f in sFilter]), sub, task, '.'.join([str(t) for t in tFilter]))\n",
    "                  for sub in subject_list\n",
    "                  for ses in session_list\n",
    "                  for task in list(tasks.keys())\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "\n",
    "    folder_new = ['sub-%s/task-%s/ses-%s/tFilter_%s_sFilter_%s/' % (\n",
    "         sub, task, ses, '.'.join([str(t) for t in tFilter]), '.'.join([str(f) for f in sFilter]))\n",
    "                  for sub in subject_list\n",
    "                  for ses in session_list\n",
    "                  for task in list(tasks.keys())\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "else:\n",
    "    \n",
    "    folder_old = ['_spatial_filt_%s_subject_id_%s_task_id_%s_temporal_filt_%s/' % (\n",
    "        '.'.join([str(f) for f in sFilter]), sub, task, '.'.join([str(t) for t in tFilter]))\n",
    "                  for sub in subject_list\n",
    "                  for task in list(tasks.keys())\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "\n",
    "    folder_new = ['sub-%s/task-%s/tFilter_%s_sFilter_%s/' % (\n",
    "         sub, task, '.'.join([str(t) for t in tFilter]), '.'.join([str(f) for f in sFilter]))\n",
    "                  for sub in subject_list\n",
    "                  for task in list(tasks.keys())\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "    \n",
    "substitutions = [z for z in zip(folder_old, folder_new)]\n",
    "substitutions += [('_gzip_con%d/' % i, '') for i in range(200)]\n",
    "substitutions += [('_gzip_con_run%d/' % i, '') for i in range(200)]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1st-Level Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "analysis_1st = Workflow(name='analysis_1st')\n",
    "analysis_1st.base_dir = work_dir\n",
    "out_folder = 'analysis_1stLevel'\n",
    "if postfix:\n",
    "    out_folder += '_%s' % postfix\n",
    "\n",
    "# Add nodes to workflow and connect them\n",
    "analysis_1st.connect([(info_source, select_files, [('subject_id', 'subject_id'),\n",
    "                                                   ('session_id', 'session_id'),\n",
    "                                                   ('task_id', 'task_id'),\n",
    "                                                   ('spatial_filt', 'sFilter'),\n",
    "                                                   ('temporal_filt', 'tFilter')]),\n",
    "                      (info_source, get_param, [('task_id', 'task_id'),\n",
    "                                                ('temporal_filt', 'tFilter')]),\n",
    "                      (select_files, get_param, [('func', 'func')]),\n",
    "                      (get_param, analysisflow, [('TR', 'model_spec.time_repetition'),\n",
    "                                                 ('high_pass', 'model_spec.high_pass_filter_cutoff'),\n",
    "                                                 ('TR', 'level1_design.interscan_interval'),\n",
    "                                                 ('contrasts', 'level1_con_est.contrasts'),\n",
    "                                                 ('condition_names', 'get_model_info.condition_names'),\n",
    "                                                ]),\n",
    "                      (get_param, reportflow, [('contrasts', 'plot_contrasts.contrast_list')]),\n",
    "                      (select_files, analysisflow, [('func', 'gunzip.in_file'),\n",
    "                                                    ('events', 'get_model_info.event_files'),\n",
    "                                                    ('nss', 'get_model_info.nss_files'),\n",
    "                                                    ('confounds', 'nuisance_reg.confounds')]),\n",
    "                      \n",
    "                      # Connect analysis and report workflow\n",
    "                      (analysisflow, reportflow, [('level1_con_est.spm_mat_file', 'plot_GLM.SPM')]),\n",
    "\n",
    "                      # Store analysis results in datasink\n",
    "                      (analysisflow, datasink, [\n",
    "                          ('level1_con_est.spm_mat_file', '%s.univariate.@spm_mat' % out_folder),\n",
    "                          ('level1_con_est.con_images', '%s.univariate.@con' % out_folder),\n",
    "                          ('level1_con_est.ess_images', '%s.univariate.@ess' % out_folder),\n",
    "                          ('level1_con_est.spmT_images', '%s.univariate.@spmT' % out_folder),\n",
    "                          ('level1_con_est.spmF_images', '%s.univariate.@spmF' % out_folder)]),\n",
    "\n",
    "                      # Store report results in datasink\n",
    "                      (reportflow, datasink, [\n",
    "                          ('plot_GLM.out_file', '%s.univariate.@spm_mat_svg' % out_folder)]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outlier parameters if requested by user\n",
    "if use_outliers:\n",
    "    analysis_1st.connect([(select_files, analysisflow, [('outliers', 'model_spec.outlier_files')])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add normalization subworkflow if requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge node\n",
    "merge = Node(Merge(2, ravel_inputs=True), name='merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of template brain with desired voxel resolution\n",
    "template_dir = '/templates/mni_icbm152_nlin_asym_09c/'\n",
    "brain_template = opj(template_dir, '1.0mm_brain.nii.gz')\n",
    "\n",
    "# Resample template brain to desired resolution\n",
    "from nibabel import load, Nifti1Image\n",
    "from nilearn.image import resample_img\n",
    "from nibabel.spaces import vox2out_vox\n",
    "\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=norm_res)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template = opj(template_dir, 'template_brain_%s.nii.gz' %'_'.join([str(n) for n in norm_res]))\n",
    "img_resample.to_filename(norm_template)\n",
    "\n",
    "# Normalize contrasts if requested\n",
    "norm_con = MapNode(ApplyTransforms(reference_image=norm_template,\n",
    "                                   input_image_type=3,\n",
    "                                   float=True,\n",
    "                                   interpolation='Linear',\n",
    "                                   invert_transform_flags=[False],\n",
    "                                   out_postfix='_norm'),\n",
    "                   name='norm_con', iterfield=['input_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gzip normalized contrasts\n",
    "def gzip_nifti(contrast):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from os.path import basename, abspath\n",
    "    \n",
    "    # Change the compression level of the NIfTI image\n",
    "    nb.openers.Opener.default_compresslevel = 6\n",
    "\n",
    "    # Save compressed contrast\n",
    "    out_file = abspath(basename(contrast.replace('.nii', '.nii.gz')))\n",
    "    nb.load(contrast).to_filename(out_file)\n",
    "\n",
    "    return out_file\n",
    "\n",
    "gzip_con = MapNode(Function(input_names=['contrast'],\n",
    "                            output_names=['out_file'],\n",
    "                            function=gzip_nifti),\n",
    "                   name='gzip_con', iterfield=['contrast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize contrasts to template space if requested by user\n",
    "if normalize:\n",
    "    \n",
    "    # Create normalization workflow\n",
    "    normflow = Workflow(name='normflow')\n",
    "    normflow.base_dir = work_dir\n",
    "    \n",
    "    # Connect nodes within normalization workflow\n",
    "    normflow.connect([(merge, norm_con, [('out', 'input_image')]),\n",
    "                      (norm_con, gzip_con, [('output_image', 'contrast')]),\n",
    "                     ])\n",
    "    \n",
    "    # Connect analysis workflow to norm subworkflow\n",
    "    analysis_1st.connect([(select_files, normflow, [('transforms', 'norm_con.transforms')]),\n",
    "                          (analysisflow, normflow, [('level1_con_est.con_images', 'merge.in1'),\n",
    "                                                    ('level1_con_est.ess_images', 'merge.in2')]),\n",
    "                          (normflow, datasink, [\n",
    "                              ('gzip_con.out_file', '%s.univariate.@norm_files' % out_folder)]),\n",
    "                          (normflow, reportflow, [('gzip_con.out_file', 'plot_contrasts.con_files')]),\n",
    "                          (reportflow, datasink, [\n",
    "                              ('plot_contrasts.out_files', '%s.univariate.@con_plots' % out_folder)]),\n",
    "                         ])\n",
    "else:\n",
    "    # Connect contrast plot node to analysis workflow\n",
    "    analysis_1st.connect([(analysisflow, merge, [('level1_con_est.con_images', 'in1'),\n",
    "                                                 ('level1_con_est.ess_images', 'in2')]),\n",
    "                          (merge, reportflow, [('out', 'plot_contrasts.con_files')]),\n",
    "                          (reportflow, datasink, [\n",
    "                              ('plot_contrasts.out_files', '%s.univariate.@con_plots' % out_folder),]),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add subworkflow for to create contrasts for multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalization workflow\n",
    "multiflow = Workflow(name='multiflow')\n",
    "multiflow.base_dir = work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrast list for condition per run\n",
    "def get_con_per_run(stimuli_order, tasks, task_id):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract condition names\n",
    "    condition_names = tasks[task_id]['condition_names']\n",
    "\n",
    "    # Aggregate event information\n",
    "    event_list = []\n",
    "    for i, l in enumerate(stimuli_order):\n",
    "        event_list.append([z for z in zip(np.full(len(l), i), l)])\n",
    "    event_info = np.reshape(event_list, (-1, 2))\n",
    "    unique_contrasts = np.unique(event_info[:,1])\n",
    "    n_runs = np.unique(event_info[:,0])\n",
    "\n",
    "    # Create list of contrasts for each condition per run\n",
    "    contrast_list_run = []\n",
    "    n_contrasts = len(condition_names)\n",
    "    n_conditions = len(n_runs)\n",
    "    condition_labels = []\n",
    "\n",
    "    for j in range(n_conditions):\n",
    "        for i in range(n_contrasts):\n",
    "            name = 'cont_%05d' % (1 + i + j * n_conditions)\n",
    "            con_id = np.zeros(n_contrasts).tolist()\n",
    "            run_id = np.zeros(n_conditions).tolist()\n",
    "            con_id[i] = 1\n",
    "            run_id[j] = 1\n",
    "            contrast_list_run.append([\n",
    "                name, 'T', condition_names, con_id, run_id])\n",
    "            condition_labels.append(condition_names[i])\n",
    "    \n",
    "    return contrast_list_run, condition_labels\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "comp_con_per_run = Node(Function(input_names=['stimuli_order', 'tasks', 'task_id'],\n",
    "                                 output_names=['run_contrasts', 'condition_labels'],\n",
    "                                 function=get_con_per_run),\n",
    "                        name='comp_con_per_run')\n",
    "comp_con_per_run.inputs.tasks = tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 1st-level contrasts - one for each session\n",
    "level1_con_est_run = Node(EstimateContrast(), name=\"level1_con_est_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample template brain for multivariate analysis to desired resolution\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=norm_res_multi)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template_multi = opj(template_dir, 'template_brain_%s.nii.gz' %'_'.join([str(n) for n in norm_res_multi]))\n",
    "img_resample.to_filename(norm_template_multi)\n",
    "\n",
    "# Normalize contrasts\n",
    "norm_con_run = MapNode(ApplyTransforms(reference_image=norm_template_multi,\n",
    "                                       input_image_type=3,\n",
    "                                       float=True,\n",
    "                                       interpolation='Linear',\n",
    "                                       invert_transform_flags=[False],\n",
    "                                       out_postfix='_norm'),\n",
    "                       name='norm_con_run', iterfield=['input_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gzip normalized contrasts\n",
    "gzip_con_run = MapNode(Function(input_names=['contrast'],\n",
    "                                output_names=['out_file'],\n",
    "                                function=gzip_nifti),\n",
    "                       name='gzip_con_run', iterfield=['contrast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write label file\n",
    "def write_labels_file(condition_labels, spm_mat_file):\n",
    "\n",
    "    import numpy as np\n",
    "    from os.path import basename, abspath\n",
    "    label_file = abspath(basename(spm_mat_file.replace('SPM.mat',\n",
    "                                                       'labels.csv')))\n",
    "    np.savetxt(label_file, condition_labels, fmt='%s')\n",
    "\n",
    "    return label_file\n",
    "\n",
    "write_labels_run = Node(Function(input_names=['condition_labels', 'spm_mat_file'],\n",
    "                                 output_names=['labels_file'],\n",
    "                                 function=write_labels_file),\n",
    "                        name='write_labels_run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow if contrasts per condition per run should be computed\n",
    "if con_per_run:\n",
    "\n",
    "    # Connect nodes within multivariate workflow\n",
    "    multiflow.connect([(comp_con_per_run, level1_con_est_run, [('run_contrasts', 'contrasts')]),\n",
    "                       (level1_con_est_run, norm_con_run, [('con_images', 'input_image')]),\n",
    "                       (norm_con_run, gzip_con_run, [('output_image', 'contrast')]),\n",
    "                       (comp_con_per_run, write_labels_run, [('condition_labels', 'condition_labels')]),\n",
    "                      ])\n",
    "\n",
    "    # Connect all nodes in this part of the workflow\n",
    "    analysis_1st.connect([(info_source, multiflow, [('task_id', 'comp_con_per_run.task_id')]),\n",
    "                          (select_files, multiflow, [('transforms', 'norm_con_run.transforms')]),\n",
    "                          (analysisflow, multiflow, [\n",
    "                              ('get_model_info.stimuli_order', 'comp_con_per_run.stimuli_order'),\n",
    "                              ('level1_estimate.spm_mat_file', 'level1_con_est_run.spm_mat_file'),\n",
    "                              ('level1_estimate.beta_images', 'level1_con_est_run.beta_images'),\n",
    "                              ('level1_estimate.residual_image', 'level1_con_est_run.residual_image'),\n",
    "                              ('level1_estimate.spm_mat_file', 'write_labels_run.spm_mat_file'),\n",
    "                          ]),\n",
    "                          (multiflow, datasink, [\n",
    "                              ('gzip_con_run.out_file', '%s.multivariate.@norm_files_run' % out_folder),\n",
    "                              ('write_labels_run.labels_file', '%s.multivariate.@norm_labels' % out_folder),\n",
    "                          ]),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create analysis_1st output graph\n",
    "analysis_1st.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(analysis_1st.base_dir, 'analysis_1st', 'graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the workflow in parallel mode\n",
    "res = analysis_1st.run(plugin='MultiProc', plugin_args={'n_procs' : n_proc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "analysis_1st.write_graph(graph2use='flat', format='png', simple_form=True)\n",
    "analysis_1st.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(analysis_1st.base_dir, 'analysis_1st', 'graph.png'),\n",
    "         opj(exp_dir, out_dir,  out_folder, 'graph.png'))\n",
    "copyfile(opj(analysis_1st.base_dir, 'analysis_1st', 'graph_detailed.png'),\n",
    "         opj(exp_dir, out_dir, out_folder, 'graph_detailed.png'));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
