{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anatomical Preprocessing\n",
    "\n",
    "This notebooks preprocesses anatomical MRI images by executing the following processing steps:\n",
    "\n",
    "1. Reorient images to RAS\n",
    "1. Crop FOV with FSL\n",
    "1. N4-inhomogenity correction with ANTS\n",
    "1. GM, WM and CSF segmentation with SPM\n",
    "1. Brainmask creation and brain extraction with Nilearn\n",
    "1. Normalization to ICBM template with ANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── analysis-anat_specs.json\n",
    "    └── sub-{sub_id}\n",
    "        └── anat\n",
    "            └── sub-{sub_id}*{T1_id}*.nii.gz\n",
    "            \n",
    "**Note:** Subfolders for individual scan sessions are optional. `fmriflows` will run the preprocessing on all files of a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `analysis-anat_specs.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'analysis-anat_specs.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)\n",
    "\n",
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject names\n",
    "subject_list = specs['subject_list']\n",
    "\n",
    "# Anatomical image identifier\n",
    "T1_id = specs['T1_id']\n",
    "\n",
    "# Resolution of normalized images\n",
    "norm_res = specs['vox_res']\n",
    "\n",
    "# Number of parallel jobs to run\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Workflow\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, Workflow, Function, IdentityInterface\n",
    "from nipype.interfaces.image import Reorient\n",
    "from nipype.interfaces.fsl import RobustFOV\n",
    "from nipype.interfaces.ants import N4BiasFieldCorrection, Registration\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.interfaces.spm import NewSegment\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-dev/spm12_mcr/spm/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fmriflows output folder if missing\n",
    "import pathlib\n",
    "pathlib.Path(opj(exp_dir, out_dir)).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of template brain with desired voxel resolution\n",
    "template_dir = '/templates/mni_icbm152_nlin_asym_09c/'\n",
    "brain_template = opj(template_dir, '1.0mm_brain.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample template brain to desired resolution\n",
    "from nibabel import load, Nifti1Image\n",
    "from nilearn.image import resample_img\n",
    "from nibabel.spaces import vox2out_vox\n",
    "\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=norm_res)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template = opj(template_dir, 'template_brain.nii.gz')\n",
    "img_resample.to_filename(norm_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient anatomical images to RAS\n",
    "reorient = Node(Reorient(orientation='RAS'), name='reorient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces FOV of images to remove lower head and neck\n",
    "cropFOV = Node(RobustFOV(output_type='NIFTI_GZ'), name='cropFOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects bias field\n",
    "n4 = Node(N4BiasFieldCorrection(dimension=3), name='n4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzips images\n",
    "gunzip = Node(Gunzip(), name='gunzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segments brain into 5 classes (GM, WM, CSF, Skull & Head)\n",
    "segment = Node(NewSegment(), name='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def get_brain_and_mask(in_file, segments):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.image import clean_img, mean_img, math_img\n",
    "    from scipy.ndimage.morphology import (\n",
    "        binary_fill_holes, binary_dilation, binary_erosion)\n",
    "\n",
    "    # Load T1w corrected image\n",
    "    img = nb.load(in_file)\n",
    "\n",
    "    # Brainmask is created from the probability tissue maps\n",
    "    gm, wm, csf, skull, head = [s[0] for s in segments]\n",
    "    img_gmwm = math_img(\"(img1 + img2) >= 0.25\", img1=gm, img2=wm)\n",
    "    img_csf = math_img(\"img1 >= 1.0\", img1=csf)\n",
    "    img_not_rest = math_img(\"(img1 + img2) >= 0.25\", img1=head, img2=skull)\n",
    "    img_mask = math_img(\"(img1 + img2 - img3) >= 1.0\", img1=img_gmwm, img2=img_csf, img3=img_not_rest)\n",
    "\n",
    "    # Improves brainmask by 1 x erosion, 2 x dilation & filling of wholes\n",
    "    data_mask = binary_erosion(\n",
    "                binary_fill_holes(\n",
    "                binary_dilation(\n",
    "                img_mask.get_data(),\n",
    "                    iterations = 2)),\n",
    "                    iterations = 1).astype('int8')\n",
    "    img_mask = nb.Nifti1Image(data_mask, img.affine, img.header)\n",
    "\n",
    "    # Extract Brain with Mask\n",
    "    img_brain = math_img(\"img1 * img2\", img1=img, img2=img_mask)\n",
    "\n",
    "    # Store output in files\n",
    "    out_file = in_file.replace('.nii', '_brain.nii')\n",
    "    mask = in_file.replace('.nii', '_brainmask.nii')\n",
    "    img_brain.to_filename(out_file)\n",
    "    img_mask.to_filename(mask)\n",
    "\n",
    "    return out_file, mask\n",
    "\n",
    "extract_brain = Node(Function(input_names=['in_file', 'segments'],\n",
    "                              output_names=['out_file', 'mask'],\n",
    "                              function=get_brain_and_mask),\n",
    "                     name='extract_brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize anatomy to ICBM template\n",
    "antsreg = Node(Registration(fixed_image=norm_template,\n",
    "                            num_threads=n_proc,\n",
    "                            output_inverse_warped_image=True,\n",
    "                            output_warped_image=True,\n",
    "\n",
    "                            collapse_output_transforms=True,\n",
    "                            dimension=3,\n",
    "                            float=True,\n",
    "                            initial_moving_transform_com=True,\n",
    "                            initialize_transforms_per_stage=False,\n",
    "                            interpolation='LanczosWindowedSinc',\n",
    "                            transforms=['Rigid', 'Affine', 'SyN'],\n",
    "                            transform_parameters=[(0.05,), (0.08,),\n",
    "                                                  (0.1, 3.0, 0.0)],\n",
    "\n",
    "                            metric=['Mattes', 'Mattes', 'CC'],\n",
    "                            metric_weight=[1.0] * 3,\n",
    "                            radius_or_number_of_bins=[56, 56, 4],\n",
    "                            sampling_strategy=['Regular', 'Regular', 'None'],\n",
    "                            sampling_percentage=[0.25, 0.25, 1],\n",
    "                            number_of_iterations=[[100, 100],\n",
    "                                                  [100, 100],\n",
    "                                                  [100, 50, 20]],\n",
    "                            convergence_threshold=[1e-06] * 3,\n",
    "                            convergence_window_size=[20, 20, 10],\n",
    "                            smoothing_sigmas=[[2, 1], [1, 0], [3, 2, 1]],\n",
    "                            sigma_units=['vox'] * 3,\n",
    "                            shrink_factors=[[2, 1], [2, 1], [8, 4, 2]],\n",
    "                            use_estimate_learning_rate_once = [True ,True, True],\n",
    "                            use_histogram_matching=True,\n",
    "\n",
    "                            winsorize_lower_quantile=0.005,\n",
    "                            winsorize_upper_quantile=0.995,\n",
    "                            write_composite_transform=True,\n",
    "                            terminal_output='file'),\n",
    "               name='antsreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Visual Report for Anatomical Preprocessing\n",
    "def write_report(sub, sess, n4, segments, brain, T1_template, warped_file):\n",
    "    \n",
    "    import os\n",
    "    import nibabel as nb\n",
    "    from nilearn.plotting import plot_stat_map, plot_roi\n",
    "    from matplotlib.pyplot import figure\n",
    "\n",
    "    import numpy as np\n",
    "    from nilearn.image import coord_transform\n",
    "    \n",
    "    # Support Function to get optimal cut for visualization\n",
    "    def get_cut_ids(img, axis=0):\n",
    "\n",
    "        # Compute voxel id to cut\n",
    "        idx = np.sort(img.get_data().nonzero()[axis])\n",
    "        vox_id = np.linspace(idx.min(), idx.max(), num=12, endpoint=True).astype('int')\n",
    "        vox_id = vox_id[2:-2]\n",
    "\n",
    "        # Translate voxel id to image space\n",
    "        if axis == 0:\n",
    "            cut_ids = [int(coord_transform(r, 0, 0, img.affine)[0]) for r in vox_id]\n",
    "        elif axis == 1:\n",
    "            cut_ids = [int(coord_transform(0, r, 0, img.affine)[1]) for r in vox_id]\n",
    "        elif axis == 2:\n",
    "            cut_ids = [int(coord_transform(0, 0, r, img.affine)[2]) for r in vox_id]\n",
    "        return cut_ids\n",
    "    \n",
    "    title_txt = 'sub: %s' % sub\n",
    "    \n",
    "    # Add session suffix if present\n",
    "    if sess:\n",
    "        title_txt += ' - sess: %s' % sess\n",
    "    \n",
    "    # Visualize Tissue Segmentation of T1w\n",
    "    img = nb.load(brain)\n",
    "    data = np.stack((np.zeros(img.get_data().shape),\n",
    "                     nb.load(segments[0][0]).get_data(),\n",
    "                     nb.load(segments[1][0]).get_data(),\n",
    "                     nb.load(segments[2][0]).get_data(),\n",
    "                     nb.load(segments[3][0]).get_data(),\n",
    "                     nb.load(segments[4][0]).get_data()), axis= -1)\n",
    "    label_id = np.argmax(data, axis=-1)\n",
    "    segmentation = nb.Nifti1Image(label_id, img.affine, img.header)\n",
    "\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "\n",
    "        plot_roi(segmentation, cmap='Accent', dim=1, annotate=False, bg_img=n4,\n",
    "                 display_mode=e, title=title_txt + ' - %s-axis' % e,\n",
    "                 resampling_interpolation='nearest',\n",
    "                 cut_coords=get_cut_ids(img, i), axes=ax)\n",
    "    \n",
    "    out_segmentation = brain.replace('brain.nii.gz', 'segmentation.svg')\n",
    "    fig.savefig(out_segmentation, bbox_inches='tight', facecolor='black',\n",
    "                frameon=True, dpi=300, transparent=True)\n",
    "\n",
    "    # Visualize Brain Extraction of T1w\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        plot_stat_map(brain, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                      threshold='auto', bg_img=n4, cmap='magma', display_mode=e,\n",
    "                      resampling_interpolation='nearest', dim=-1,\n",
    "                      cut_coords=get_cut_ids(nb.load(brain), i), annotate=False, axes=ax)\n",
    "\n",
    "    out_brain = brain.replace('.nii.gz', '.svg')\n",
    "    fig.savefig(out_brain, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=True)\n",
    "    \n",
    "    # Visualize T1w to MNI registration\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        plot_stat_map(warped_file, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                      threshold='auto', bg_img=T1_template, display_mode=e,\n",
    "                      resampling_interpolation='nearest',\n",
    "                      cut_coords=get_cut_ids(nb.load(warped_file), i),\n",
    "                      cmap='magma', annotate=False, axes=ax)\n",
    "    \n",
    "    out_warp = warped_file.replace('.nii.gz', '.svg')\n",
    "    fig.savefig(out_warp, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=True)\n",
    "    \n",
    "    # Write the HTML report\n",
    "    with open('/templates/report_template.html', 'r') as report:\n",
    "        txt = report.read()\n",
    "        txt = txt.replace('sub-placeholder', 'sub-%s' % sub)\n",
    "        \n",
    "        # Add session suffix if present\n",
    "        if sess:\n",
    "            txt = txt.replace('ses-placeholder', 'ses-%s' % sess)\n",
    "            filename = 'sub-%s_ses-%s.html' % (sub, sess)\n",
    "        else:\n",
    "            txt = txt.replace('ses-placeholder', '')\n",
    "            txt = txt.replace('__', '_')\n",
    "            filename = 'sub-%s.html' % sub\n",
    "\n",
    "    report_file = os.path.join('/data', 'derivatives', 'fmriflows', filename)\n",
    "    \n",
    "    with open(report_file, 'w') as report:\n",
    "        report.writelines(txt)\n",
    "\n",
    "    return out_segmentation, out_brain, out_warp\n",
    "    \n",
    "# Create Report Node\n",
    "create_report = Node(Function(input_names=['sub', 'sess', 'n4', 'segments', 'brain',\n",
    "                                           'T1_template', 'warped_file'],\n",
    "                              output_names=['out_segmentation', 'out_brain', 'out_warp'],\n",
    "                              function=write_report),\n",
    "                name='create_report')\n",
    "create_report.inputs.T1_template = brain_template.replace('brain', 'T1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all anatomical files\n",
    "from bids.grabbids import BIDSLayout\n",
    "layout = BIDSLayout('/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get session name if it exists\n",
    "session_list = layout.get_sessions()\n",
    "session_list = session_list if session_list else ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject and session id\n",
    "infosource = Node(IdentityInterface(fields=['subject_id', 'session_id']),\n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('session_id', session_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def create_file_path(subject_id, session_id, layout, T1_id):\n",
    "\n",
    "    from os.path import join\n",
    "    \n",
    "    entities = {'subject_id': subject_id,\n",
    "                'T1_id': T1_id}\n",
    "    \n",
    "    # Add session id if present in dataset\n",
    "    if session_id != '':\n",
    "        entities['session_id'] = session_id\n",
    "    \n",
    "    pattern = 'sub-{subject_id}[/ses-{session_id}]/anat/'\n",
    "    pattern += 'sub-{subject_id}[_ses-{session_id}]_{T1_id}.nii.gz'\n",
    "\n",
    "    fpath = layout.build_path(entities, path_patterns=[pattern])\n",
    "\n",
    "    return join('/data', fpath)\n",
    "\n",
    "selectfiles = Node(Function(input_names=['subject_id', 'session_id',\n",
    "                                         'layout', 'T1_id'],\n",
    "                            output_names=['anat'],\n",
    "                            function=create_file_path),\n",
    "                   name='selectfiles')\n",
    "selectfiles.inputs.layout = layout\n",
    "selectfiles.inputs.T1_id = T1_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [('_session_id_%s_subject_id_%s/' % (sess, sub),\n",
    "                  'sub-%s/sub-%s_ses-%s_' % (sub, sub, sess))\n",
    "                 for sess in session_list\n",
    "                 for sub in subject_list]\n",
    "substitutions += [('_ras', ''),\n",
    "                  ('_ROI', ''),\n",
    "                  ('_%s_corrected' % T1_id, ''),\n",
    "                  ('c1', 'seg_gm_'),\n",
    "                  ('c2', 'seg_wm_'),\n",
    "                  ('c3', 'seg_csf_'),\n",
    "                  ('c4', 'seg_skull_'),\n",
    "                  ('c5', 'seg_head_'),\n",
    "                  ('ses-_', ''),\n",
    "                 ]\n",
    "substitutions += [('sub-%s_sub-%s' % (sub, sub), 'sub-%s' % sub)\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('_sub-%s_ses-%s' % (sub, sess), '')\n",
    "                  for sess in session_list\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('_sub-%s.nii' % sub, '.nii')\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('/sub-%s_ses-%s.nii' % (sub, sess),\n",
    "                   '/sub-%s_ses-%s_T1w_corrected.nii' % (sub, sess))\n",
    "                  for sess in session_list\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('/sub-%s.nii' % sub,\n",
    "                   '/sub-%s_T1w_corrected.nii' % sub)\n",
    "                  for sess in session_list\n",
    "                  for sub in subject_list]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "preproc_anat = Workflow(name='preproc_anat')\n",
    "preproc_anat.base_dir = work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "preproc_anat.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                 ('session_id', 'session_id')]),\n",
    "\n",
    "                      # Main part of workflow\n",
    "                      (selectfiles, reorient, [('anat', 'in_file')]),\n",
    "                      (reorient, cropFOV, [('out_file', 'in_file')]),\n",
    "                      (cropFOV, n4, [('out_roi', 'input_image')]),\n",
    "                      (n4, gunzip, [('output_image', 'in_file')]),\n",
    "                      (gunzip, segment, [('out_file', 'channel_files')]),\n",
    "                      (segment, extract_brain, [('native_class_images', 'segments')]),\n",
    "                      (n4, extract_brain, [('output_image', 'in_file')]),\n",
    "                      (extract_brain, antsreg, [('out_file', 'moving_image')]),\n",
    "\n",
    "                      # Store main results in datasink\n",
    "                      (n4, datasink, [('output_image', 'preproc_anat.@n4')]),\n",
    "                      (segment, datasink, [\n",
    "                          ('native_class_images', 'preproc_anat.@segment')]),\n",
    "                      (extract_brain, datasink, [('out_file', 'preproc_anat.@brain'),\n",
    "                                                 ('mask', 'preproc_anat.@mask')]),\n",
    "                      (antsreg, datasink, [\n",
    "                          ('warped_image', 'preproc_anat.@warped_image'),\n",
    "                          ('inverse_warped_image', 'preproc_anat.@inverse_warped_image'),\n",
    "                          ('composite_transform', 'preproc_anat.@transform'),\n",
    "                          ('inverse_composite_transform', 'preproc_anat.@inverse_transform')]),\n",
    "\n",
    "                      # Create visual report\n",
    "                      (infosource, create_report, [('subject_id', 'sub'),\n",
    "                                                   ('session_id', 'sess')]),\n",
    "                      (n4, create_report, [('output_image', 'n4')]),\n",
    "                      (segment, create_report, [('native_class_images', 'segments')]),\n",
    "                      (extract_brain, create_report, [('out_file', 'brain')]),\n",
    "                      (antsreg, create_report, [('warped_image', 'warped_file')]),\n",
    "                      (create_report, datasink, [('out_segmentation', 'preproc_anat.@vis_segmentation'),\n",
    "                                                 ('out_brain', 'preproc_anat.@vis_brain'),\n",
    "                                                 ('out_warp', 'preproc_anat.@vis_warp')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproc_anat output graph\n",
    "preproc_anat.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook\n",
    "from IPython.display import SVG\n",
    "SVG(filename=opj(preproc_anat.base_dir, 'preproc_anat', 'graph.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow in sequential mode\n",
    "preproc_anat.run('Linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "preproc_anat.write_graph(graph2use='flat', format='svg', simple_form=True)\n",
    "preproc_anat.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph.svg'))\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph_detailed.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph_detailed.svg'));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
