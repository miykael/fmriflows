{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anatomical Preprocessing\n",
    "\n",
    "This notebooks preprocesses anatomical MRI images by executing the following processing steps:\n",
    "\n",
    "1. Reorient images to RAS\n",
    "1. Crop FOV with FSL\n",
    "1. N4-inhomogenity correction with ANTS\n",
    "1. GM, WM and CSF segmentation with SPM\n",
    "1. Brainmask creation and brain extraction with Nilearn\n",
    "1. Normalization to ICBM template with ANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── analysis-anat_specs.json\n",
    "    └── sub-{sub_id}\n",
    "        └── anat\n",
    "            └── sub-{sub_id}*{T1w_id}*.nii.gz\n",
    "            \n",
    "**Note:** Subfolders for individual scan sessions are optional. `fmriflows` will run the preprocessing on all files of a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `fmriflows_spec_preproc.json` file in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_preproc.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for anatomical preprocessing workflow\n",
    "subject_list = specs['subject_list_anat']\n",
    "session_list = specs['session_list_anat']\n",
    "T1w_id = specs['T1w_id']\n",
    "res_norm = specs['res_norm']\n",
    "n_stages = specs['steps_of_norm']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anatomical image identifier\n",
    "T1w_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution of normalized images\n",
    "res_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 or 3 steps ANTs normalization\n",
    "n_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Workflow\n",
    "\n",
    "To ensure a good overview of the anatomical preprocessing, the workflow was divided into two subworkflows:\n",
    "\n",
    "1. The Main Workflow, i.e. doing the actual preprocessing\n",
    "2. Report Workflow, i.e. visualizating relevant steps for quality control\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, Workflow, Function, IdentityInterface\n",
    "from nipype.interfaces.image import Reorient\n",
    "from nipype.interfaces.fsl import RobustFOV\n",
    "from nipype.interfaces.ants import N4BiasFieldCorrection, Registration\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.interfaces.spm import NewSegment\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-r7219/spm12_mcr/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fmriflows output folder if missing\n",
    "import pathlib\n",
    "pathlib.Path(opj(exp_dir, out_dir)).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of template brain with desired voxel resolution\n",
    "template_dir = '/templates/mni_icbm152_nlin_asym_09c/'\n",
    "brain_template = opj(template_dir, '1.0mm_brain.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample template brain to desired resolution\n",
    "from nibabel import load, Nifti1Image\n",
    "from nilearn.image import resample_img\n",
    "from nibabel.spaces import vox2out_vox\n",
    "\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=res_norm)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template = opj(template_dir, 'template_brain_%s.nii.gz' %'_'.join([str(n) for n in res_norm]))\n",
    "img_resample.to_filename(norm_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Main Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient anatomical images to RAS\n",
    "reorient = Node(Reorient(orientation='RAS'), name='reorient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces FOV of images to remove lower head and neck\n",
    "cropFOV = Node(RobustFOV(output_type='NIFTI_GZ'), name='cropFOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects bias field\n",
    "n4 = Node(N4BiasFieldCorrection(dimension=3), name='n4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzips images\n",
    "gunzip = Node(Gunzip(), name='gunzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segments brain into 5 classes (GM, WM, CSF, Skull & Head)\n",
    "segment = Node(NewSegment(), name='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def get_brain_and_mask(in_file, segments):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.image import clean_img, mean_img, math_img\n",
    "    from scipy.ndimage.morphology import (\n",
    "        binary_fill_holes, binary_dilation, binary_erosion)\n",
    "    from os.path import basename, abspath\n",
    "\n",
    "    # Load T1w corrected image\n",
    "    img = nb.load(in_file)\n",
    "\n",
    "    # Brainmask is created from the probability tissue maps\n",
    "    gm, wm, csf, skull, head = [s[0] for s in segments]\n",
    "    img_gmwm = math_img(\"(img1 + img2) >= 0.25\", img1=gm, img2=wm)\n",
    "    img_csf = math_img(\"img1 >= 1.0\", img1=csf)\n",
    "    img_not_rest = math_img(\"(img1 + img2) >= 0.25\", img1=head, img2=skull)\n",
    "    img_mask = math_img(\"(img1 + img2 - img3) >= 1.0\", img1=img_gmwm, img2=img_csf, img3=img_not_rest)\n",
    "\n",
    "    # Improves brainmask by 1 x erosion, 2 x dilation & filling of wholes\n",
    "    data_mask = binary_erosion(\n",
    "                binary_fill_holes(\n",
    "                binary_dilation(\n",
    "                img_mask.get_data(),\n",
    "                    iterations = 2)),\n",
    "                    iterations = 1).astype('int8')\n",
    "    img_mask = nb.Nifti1Image(data_mask, img.affine, img.header)\n",
    "\n",
    "    # Extract Brain with Mask\n",
    "    img_brain = math_img(\"img1 * img2\", img1=img, img2=img_mask)\n",
    "\n",
    "    # Store output in nifti files\n",
    "    filename = abspath(basename(in_file))\n",
    "    out_file = filename.replace('.nii', '_brain.nii')\n",
    "    mask = filename.replace('.nii', '_brainmask.nii')\n",
    "    img_brain.to_filename(out_file)\n",
    "    img_mask.to_filename(mask)\n",
    "\n",
    "    return out_file, mask\n",
    "\n",
    "extract_brain = Node(Function(input_names=['in_file', 'segments'],\n",
    "                              output_names=['out_file', 'mask'],\n",
    "                              function=get_brain_and_mask),\n",
    "                     name='extract_brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress segmentation files\n",
    "def compress_segments(segments):\n",
    "\n",
    "    import nibabel as nb\n",
    "    from os.path import basename, abspath\n",
    "    \n",
    "    # Change the compression level of the NIfTI image\n",
    "    nb.openers.Opener.default_compresslevel = 6\n",
    "\n",
    "    # Go through the individual segments\n",
    "    compressed_segments = []\n",
    "    for s in segments:\n",
    "        new_fname  = s[0] + '.gz'\n",
    "        nb.load(s[0]).to_filename(new_fname)\n",
    "        compressed_segments.append(new_fname)\n",
    "    \n",
    "    return compressed_segments\n",
    "\n",
    "compressor = Node(Function(input_names=['segments'],\n",
    "                           output_names=['compressed_segments'],\n",
    "                           function=compress_segments),\n",
    "                  name='compressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize anatomy to ICBM template\n",
    "antsreg = Node(Registration(fixed_image=norm_template,\n",
    "                            num_threads=1,\n",
    "                            output_inverse_warped_image=True,\n",
    "                            output_warped_image=True,\n",
    "\n",
    "                            collapse_output_transforms=True,\n",
    "                            dimension=3,\n",
    "                            float=True,\n",
    "                            initial_moving_transform_com=True,\n",
    "                            interpolation='Linear',\n",
    "                            transforms=['Rigid', 'Affine', 'SyN'][:n_stages],\n",
    "                            transform_parameters=[(0.1,), (0.08,),\n",
    "                                                  (0.1, 3.0, 0.0)][:n_stages],\n",
    "\n",
    "                            metric=['Mattes', 'Mattes', 'CC'][:n_stages],\n",
    "                            metric_weight=[1.0] * n_stages,\n",
    "                            radius_or_number_of_bins=[64, 64, 4][:n_stages],\n",
    "                            sampling_strategy=['Regular', 'Regular', 'None'][:n_stages],\n",
    "                            sampling_percentage=[0.25, 0.25, 1][:n_stages],\n",
    "                            number_of_iterations=[[1000, 500, 250, 100],\n",
    "                                                  [1000, 500, 250, 100],\n",
    "                                                  [100, 75, 50, 25]][:n_stages],\n",
    "                            convergence_threshold=[1e-06] * n_stages,\n",
    "                            convergence_window_size=[20, 20, 10][:n_stages],\n",
    "                            smoothing_sigmas=[[3, 2, 1, 0]] * n_stages,\n",
    "                            sigma_units=['vox'] * n_stages,\n",
    "                            shrink_factors=[[8, 4, 2, 1]] * n_stages,\n",
    "                            use_estimate_learning_rate_once=[True ,True, True][:n_stages],\n",
    "                            use_histogram_matching=True,\n",
    "\n",
    "                            winsorize_lower_quantile=0.005,\n",
    "                            winsorize_upper_quantile=0.995,\n",
    "                            write_composite_transform=True,\n",
    "                            verbose=True,\n",
    "                            terminal_output='file'),\n",
    "               name='antsreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main preprocessing workflow\n",
    "mainflow = Workflow(name='mainflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "mainflow.connect([(reorient, cropFOV, [('out_file', 'in_file')]),\n",
    "                  (cropFOV, n4, [('out_roi', 'input_image')]),\n",
    "                  (n4, gunzip, [('output_image', 'in_file')]),\n",
    "                  (gunzip, segment, [('out_file', 'channel_files')]),\n",
    "                  (segment, extract_brain, [('native_class_images', 'segments')]),\n",
    "                  (segment, compressor, [('native_class_images', 'segments')]),\n",
    "                  (n4, extract_brain, [('output_image', 'in_file')]),\n",
    "                  (extract_brain, antsreg, [('out_file', 'moving_image')])\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the report Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual figures for anatomical preprocessing\n",
    "def plot_figures(sub, sess, n4, segments, brain, T1_template, warped_file):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.plotting import plot_stat_map, plot_roi\n",
    "    from matplotlib.pyplot import figure\n",
    "\n",
    "    import numpy as np\n",
    "    from nilearn.image import coord_transform\n",
    "    from os.path import basename, abspath\n",
    "    \n",
    "    # Support Function to get optimal cut for visualization\n",
    "    def get_cut_ids(img, axis=0):\n",
    "\n",
    "        # Compute voxel id to cut\n",
    "        idx = np.sort(img.get_data().nonzero()[axis])\n",
    "        vox_id = np.linspace(idx.min(), idx.max(), num=12, endpoint=True).astype('int')\n",
    "        vox_id = vox_id[2:-2]\n",
    "\n",
    "        # Translate voxel id to image space\n",
    "        if axis == 0:\n",
    "            cut_ids = [int(coord_transform(r, 0, 0, img.affine)[0]) for r in vox_id]\n",
    "        elif axis == 1:\n",
    "            cut_ids = [int(coord_transform(0, r, 0, img.affine)[1]) for r in vox_id]\n",
    "        elif axis == 2:\n",
    "            cut_ids = [int(coord_transform(0, 0, r, img.affine)[2]) for r in vox_id]\n",
    "        return cut_ids\n",
    "    \n",
    "    title_txt = 'sub: %s' % sub\n",
    "    \n",
    "    # Add session suffix if present\n",
    "    if sess:\n",
    "        title_txt += ' - sess: %s' % sess\n",
    "    \n",
    "    # Visualize Tissue Segmentation of T1w\n",
    "    img = nb.load(brain)\n",
    "    data = np.stack((np.zeros(img.get_data().shape),\n",
    "                     nb.load(segments[0][0]).get_data(),\n",
    "                     nb.load(segments[1][0]).get_data(),\n",
    "                     nb.load(segments[2][0]).get_data(),\n",
    "                     nb.load(segments[3][0]).get_data(),\n",
    "                     nb.load(segments[4][0]).get_data()), axis= -1)\n",
    "    label_id = np.argmax(data, axis=-1)\n",
    "    segmentation = nb.Nifti1Image(label_id, img.affine, img.header)\n",
    "\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "\n",
    "        plot_roi(segmentation, cmap='Accent', dim=1, annotate=False, bg_img=n4,\n",
    "                 display_mode=e, title=title_txt + ' - %s-axis' % e,\n",
    "                 resampling_interpolation='nearest',\n",
    "                 cut_coords=get_cut_ids(img, i), axes=ax)\n",
    "    \n",
    "    out_segmentation = basename(brain).replace('brain.nii.gz', 'segmentation.svg')\n",
    "    fig.savefig(out_segmentation, bbox_inches='tight', facecolor='black',\n",
    "                frameon=True, dpi=300, transparent=True)\n",
    "\n",
    "    # Visualize Brain Extraction of T1w\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        plot_stat_map(brain, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                      threshold='auto', bg_img=n4, cmap='magma', display_mode=e,\n",
    "                      resampling_interpolation='nearest', dim=-1,\n",
    "                      cut_coords=get_cut_ids(nb.load(brain), i), annotate=False, axes=ax)\n",
    "\n",
    "    out_brain = basename(brain).replace('.nii.gz', '.svg')\n",
    "    fig.savefig(out_brain, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=True)\n",
    "    \n",
    "    # Visualize T1w to MNI registration\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        plot_stat_map(warped_file, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                      threshold='auto', bg_img=T1_template, display_mode=e,\n",
    "                      resampling_interpolation='nearest',\n",
    "                      cut_coords=get_cut_ids(nb.load(warped_file), i),\n",
    "                      cmap='magma', annotate=False, axes=ax)\n",
    "    \n",
    "    out_warp = basename(warped_file).replace('.nii.gz', '.svg')\n",
    "    fig.savefig(out_warp, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=True)\n",
    "    \n",
    "    return abspath(out_segmentation), abspath(out_brain), abspath(out_warp), sub, sess\n",
    "    \n",
    "# Create Plotting Node\n",
    "create_figures = Node(Function(input_names=['sub', 'sess', 'n4', 'segments', 'brain',\n",
    "                                           'T1_template', 'warped_file'],\n",
    "                              output_names=['out_segmentation', 'out_brain', 'out_warp',\n",
    "                                            'sub', 'sess'],\n",
    "                              function=plot_figures),\n",
    "                name='create_figures')\n",
    "create_figures.inputs.T1_template = brain_template.replace('brain', 'T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the HTML report\n",
    "def write_report(sub, sess):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    with open('/templates/report_template.html', 'r') as report:\n",
    "        txt = report.read()\n",
    "        txt = txt.replace('sub-placeholder', 'sub-%s' % sub)\n",
    "        \n",
    "        # Add session suffix if present\n",
    "        if sess:\n",
    "            txt = txt.replace('ses-placeholder', 'ses-%s' % sess)\n",
    "            filename = 'sub-%s_ses-%s.html' % (sub, sess)\n",
    "        else:\n",
    "            txt = txt.replace('ses-placeholder', '')\n",
    "            txt = txt.replace('__', '_')\n",
    "            filename = 'sub-%s.html' % sub\n",
    "\n",
    "    report_file = os.path.join('/data', 'derivatives', 'fmriflows', filename)\n",
    "    \n",
    "    with open(report_file, 'w') as report:\n",
    "        report.writelines(txt)\n",
    "\n",
    "# Create Report Node\n",
    "create_report = Node(Function(input_names=['sub', 'sess'],\n",
    "                              function=write_report),\n",
    "                     name='create_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create report Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create report workflow\n",
    "reportflow = Workflow(name='reportflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "reportflow.connect([(create_figures, create_report, [('sub', 'sub'),\n",
    "                                                     ('sess', 'sess')\n",
    "                                                     ])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all anatomical files\n",
    "from bids.layout import BIDSLayout\n",
    "layout = BIDSLayout('/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject and session id\n",
    "infosource = Node(IdentityInterface(fields=['subject_id', 'session_id']),\n",
    "                  name='infosource')\n",
    "if session_list:\n",
    "    infosource.iterables = [('subject_id', subject_list),\n",
    "                            ('session_id', session_list)]\n",
    "else:\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.session_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def create_file_path(subject_id, session_id, layout, T1w_id):\n",
    "\n",
    "    from os.path import join\n",
    "    \n",
    "    entities = {'subject_id': subject_id,\n",
    "                'T1w_id': T1w_id}\n",
    "    \n",
    "    # Add session id if present in dataset\n",
    "    if session_id != '':\n",
    "        entities['session_id'] = session_id\n",
    "    \n",
    "    pattern = 'sub-{subject_id}[/ses-{session_id}]/anat/'\n",
    "    pattern += 'sub-{subject_id}[_ses-{session_id}]_{T1w_id}.nii.gz'\n",
    "\n",
    "    fpath = layout.build_path(entities, path_patterns=[pattern])\n",
    "\n",
    "    return join('/data', fpath)\n",
    "\n",
    "selectfiles = Node(Function(input_names=['subject_id', 'session_id',\n",
    "                                         'layout', 'T1w_id'],\n",
    "                            output_names=['anat'],\n",
    "                            function=create_file_path),\n",
    "                   name='selectfiles')\n",
    "selectfiles.inputs.layout = layout\n",
    "selectfiles.inputs.T1w_id = T1w_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [('_subject_id_%s/' % (sub),\n",
    "                  'sub-{0}/sub-{0}_'.format(sub))\n",
    "                 for sub in subject_list]\n",
    "substitutions += [('_session_id_%s_subject_id_%s/' % (sub, sess),\n",
    "                   'sub-{0}/sub-{0}__ses-{1}_'.format(sub, sess))\n",
    "                  for sub in subject_list\n",
    "                  for sess in session_list]\n",
    "substitutions += [('_%s' % T1w_id, ''),\n",
    "                  ('_ras', ''),\n",
    "                  ('_ROI', ''),\n",
    "                  ('_corrected', ''),\n",
    "                  ('c1', 'seg_gm_'),\n",
    "                  ('c2', 'seg_wm_'),\n",
    "                  ('c3', 'seg_csf_'),\n",
    "                  ('c4', 'seg_skull_'),\n",
    "                  ('c5', 'seg_head_')\n",
    "                 ]\n",
    "substitutions += [('sub-{0}_sub-{0}'.format(sub), 'sub-%s' % sub)\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('_sub-%s_ses-%s' % (sub, sess), '')\n",
    "                  for sess in session_list\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('_sub-%s.nii' % sub, '.nii')\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('/sub-%s_ses-%s.nii' % (sub, sess),\n",
    "                   '/sub-%s_ses-%s_T1w_corrected.nii' % (sub, sess))\n",
    "                  for sess in session_list\n",
    "                  for sub in subject_list]\n",
    "substitutions += [('/sub-%s.nii' % sub,\n",
    "                   '/sub-%s_T1w_corrected.nii' % sub)\n",
    "                  for sub in subject_list]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "preproc_anat = Workflow(name='preproc_anat')\n",
    "preproc_anat.base_dir = work_dir\n",
    "\n",
    "preproc_anat.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                 ('session_id', 'session_id')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the main workflow\n",
    "preproc_anat.connect([(selectfiles, mainflow, [('anat', 'reorient.in_file')]),\n",
    "                      \n",
    "                       (mainflow, datasink, [\n",
    "                           ('n4.output_image', 'preproc_anat.@n4'),\n",
    "                           ('compressor.compressed_segments', 'preproc_anat.@segment'),\n",
    "                           ('extract_brain.out_file', 'preproc_anat.@brain'),\n",
    "                           ('extract_brain.mask', 'preproc_anat.@mask'),\n",
    "                           ('antsreg.warped_image', 'preproc_anat.@warped_image'),\n",
    "                           ('antsreg.inverse_warped_image', 'preproc_anat.@inverse_warped_image'),\n",
    "                           ('antsreg.composite_transform', 'preproc_anat.@transform'),\n",
    "                           ('antsreg.inverse_composite_transform', 'preproc_anat.@inverse_transform')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the report workflow\n",
    "preproc_anat.connect([(infosource, reportflow, [('subject_id', 'create_figures.sub'),\n",
    "                                                ('session_id', 'create_figures.sess')\n",
    "                                               ]),\n",
    "\n",
    "                      (reportflow, datasink, [\n",
    "                          ('create_figures.out_segmentation', 'preproc_anat.@vis_segmentation'),\n",
    "                          ('create_figures.out_brain', 'preproc_anat.@vis_brain'),\n",
    "                          ('create_figures.out_warp', 'preproc_anat.@vis_warp'),\n",
    "                      ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect main workflow with report workflow\n",
    "preproc_anat.connect([(mainflow, reportflow, [\n",
    "                        ('n4.output_image', 'create_figures.n4'),\n",
    "                        ('segment.native_class_images', 'create_figures.segments'),\n",
    "                        ('extract_brain.out_file', 'create_figures.brain'),\n",
    "                        ('antsreg.warped_image', 'create_figures.warped_file')\n",
    "                        ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproc_anat output graph\n",
    "preproc_anat.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook\n",
    "from IPython.display import SVG\n",
    "SVG(filename=opj(preproc_anat.base_dir, 'preproc_anat', 'graph.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the workflow in sequential mode\n",
    "res = preproc_anat.run('Linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "preproc_anat.write_graph(graph2use='flat', format='svg', simple_form=True)\n",
    "preproc_anat.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph.svg'))\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph_detailed.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph_detailed.svg'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save template brain in `preproc_anat` folder\n",
    "from  os.path import basename\n",
    "new_path = '/data/derivatives/fmriflows/preproc_anat/%s' % basename(norm_template)\n",
    "\n",
    "import shutil\n",
    "shutil.move(norm_template, new_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
